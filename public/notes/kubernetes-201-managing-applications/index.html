<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Kubernetes 201: Managing Apps |
Agus Fahmi Aji Pramana's Blogging Site
</title><meta name=keywords content><meta name=description content="
    Managing Applications with Kubernetes Notes"><meta name=author content="Agus Fahmi Aji Pramana"><link rel=canonical href=http://localhost:1313/notes/kubernetes-201-managing-applications/><link crossorigin=anonymous href=/assets/css/stylesheet.606be4c20b063263aae99830f89a0f4bbedcc98a7b8fea0b216f60439a69a571.css integrity="sha256-YGvkwgsGMmOq6Zgw+JoPS77cyYp7j+oLIW9gQ5pppXE=" rel="preload stylesheet" as=style><link crossorigin=anonymous referrerpolicy=no-referrer rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css integrity="sha512-z3gLpd7yknf1YoNbCzqRKc4qyor8gaKU1qmn+CShxbuBusANI9QpRohGBreCFkKxLhei6S9CQXFEbbKuqLg0DA=="><link rel=icon href=http://localhost:1313/favicon.ico type=image/x-icon><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=manifest href=/site.webmanifest><link rel=icon type=image/png sizes=192x192 href=http://localhost:1313/img/android-chrome-192x192.png><link rel=icon type=image/png sizes=512x512 href=http://localhost:1313/img/android-chrome-512x512.png><link rel=apple-touch-icon href=http://localhost:1313/img/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg color=#2e2e33><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33
  "><link rel=alternate hreflang=en href=http://localhost:1313/notes/kubernetes-201-managing-applications/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Kubernetes 201: Managing Apps"><meta property="og:description" content="Managing Applications with Kubernetes Notes"><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/notes/kubernetes-201-managing-applications/"><meta property="og:image" content="http://localhost:1313/logo.svg"><meta property="article:section" content="notes"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/logo.svg"><meta name=twitter:title content="Kubernetes 201: Managing Apps"><meta name=twitter:description content="Managing Applications with Kubernetes Notes"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Personal Notes, Learning, Self-Preparation","item":"http://localhost:1313/notes/"},{"@type":"ListItem","position":2,"name":"Kubernetes 201: Managing Apps","item":"http://localhost:1313/notes/kubernetes-201-managing-applications/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Kubernetes 201: Managing Apps","name":"Kubernetes 201: Managing Apps","description":"Managing Applications with Kubernetes Notes","keywords":[],"articleBody":"ReplicaSet ensures the right number of pods are always up and running provide high availability through redundancy adds or deletes pods for scaling always tries to match the actual state of the replicas to the desired state replaces failing pods or deletes additional pods to maintain the desired state supersedes ReplicaControllers Deployments manage ReplicaSets, send pods declarative updates Create ReplicaSet from scratch # replicaset.yaml apiVersion: apps/v1 kind: ReplicaSet metadata: name: hello-kubernetes spec: replicas: 1 selector: matchLabels: app: hello-kubernetes template: metadata: labels: app: hello-kubernetes spec: containers: - name: hello-kubernetes image: paulbouwer/hello-kubernets:1.5 ports: -containerPort: 8080 # Outputs ReplicaSet was created kubectl create -f replicaset.yaml # Confirm by using \"get pods\" kubectl get pods # Output shows details about ReplicaSet and pod created kubectl get rs Note: Creating a Deployment that includes a ReplicaSet is recommended over creating a standalone ReplicaSet.\n# Create deployment kubectl create -f deployment.yaml kubectl get pods kubectl get deploy # Scale deployment kubectl scale deploy hello-kubernetes --replicas=3 # Should shows 3 pods running kubectl get pods # Delete pod kubectl delete pod hello-kubernetes-2324343-5mflw # Here desired state (--replicas=3) does not # match the actual state [running pods=2] # and deleted pod replaced by new one # Create pod extra pod kubectl create pod hello-kubernetes-122323 # Here desired state (--replicas=3) does not # match the actual state [running pods=4] # and new pod is marked for deletion and removed automatically Autoscaling ReplicaSets provide a good start for scaling, but you don’t always want 10 instances of your resource running. Kubernetes autoscaling helps optimize resource usage and costs by automatically scaling a cluster in line with demand as needed. Kubernetes enables autoscaling at two different layers: the cluster or node level the pod level. Three types of autoscalers are available in Kubernetes: The Horizontal Pod Autoscaler (or HPA) Vertical Pod Autoscaler (or VPA) Cluster Autoscaler (or CA) To create autoscaling:\nkubectl get pods kubectl get rs kubectl autoscale deploy hello-kubernetes --min=2 --max=5 --cpu-percent=50 kubectl describe rs hello-kubernetes- List the current number or state of pods. A ReplicaSet is automatically created when you create a deployment. In order to autoscale, you simply use the autoscale command with the requisite attributes. Min is the number of minimum pods – notice that we have changed the value of “Min” to 2. Max is the number of maximum pods. CPU-percent acts as a trigger that tells the system to create a new pod when the CPU usage reaches 50% across the cluster. In the background, the deployment still uses the ReplicaSet to scale up and down. The describe command shows the number of replicas in the “autoscaled” ReplicaSet. The Horizontal Pod Autoscaler (or HPA) adjusts the number of replicas of an application by increasing or decreasing the number of pods. automatically updates a workload resource (like a deployment) by horizontally scaling the workload to match the demand Horizontal scaling, or “scaling out,” automatically increases or decreases the number of running pods as application usage changes. uses a cluster operator that sets targets for metrics like CPU or memory utilization and the maximum and minimum desired number of replicas even though you can create an HPA autoscaler from scratch, you should use the autoscale command instead. kubectl get hpa For example,\nthe system load is low early in the morning, so one pod is sufficient. The HPA autoscales the workload resource to meet usage demand. By 11am, peak load drives a need for three pods, so the HPA autoscales the workload resource to meet usage demand. Usage drops in the afternoon, so the third pod is marked for deletion and removed. And usage drops even lower by 5pm, so another pod is marked for deletion and removed. # another way to enable autoscaling is to manually # create the HPA object from a YAML file. apiVersion: autoscaling/v1 kind: HorizontalPodAutoScaler metadata: name: hello-kubernetes namespace: default spec: # can set the minimum and maximum number of pods. maxReplicas: 5 minReplicas: 2 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: hello-kubernetes # The CPU-percent flag shows up as “targetCPUUtilizationPercentage”. targetCPUUtilizationPercentage: 10 Vertical Pod Autoscaler (or VPA) adjusts the resource requests and limits of a container by increasing or decreasing the resource size or speed of the pods. A best practice is to scale horizontally, but there are some services you may want to run in a cluster where horizontal scaling is impossible or not ideal. Vertical scaling, or “scaling up,” refers to adding more resources to an existing machine. lets you scale a service vertically within a cluster. uses a cluster operator sets targets for metrics like CPU or memory utilization, similar to an HPA. The cluster then reconciles the size of the service’s pod or pods based on their current usage and the desired target. For example,\nthe system load is low early in the morning, so system resources used by the pod are low. By 11am, peak load drives a need for more capacity. The VPA autoscales the pod by adding more system resources (CPU and memory) to meet the demand. Usage drops in the afternoon, so the pod is autoscaled to use fewer system resources. And usage drops even lower by 5pm, so the pod is autoscaled further to match the 7am levels. Note: You should not use VPAs with HPAs on resource metrics like CPU or memory. However, you can use them together on custom or external metrics.\nCluster Autoscaler (or CA) adjusts the number of nodes in the cluster when pods fail to schedule, or demand increases or decreases in relation to the existing nodes’ capacity autoscales the cluster itself, increasing and decreasing the number of available nodes that pods can run on. Pods are autoscaled using HPA or VPA, but when the nodes themselves are overloaded with pods. can use a CA to autoscale the nodes so that the pods can rebalance themselves across the cluster. For example,\nthe system load is low early in the morning, so existing nodes can handle the load.\nWhen demand increases, new pod requests come in, and the CA autoscales the cluster by adding a new node and pods to meet the demand.\nBy 11 am, peak load brings the new node to full capacity.\nWhen usage drops in the afternoon, unused pods are marked for deletion and removed.\nAnd when usage drops even lower by 5 pm, all pods in the new node are marked for deletion and removed.\nAnd then the node itself is marked and removed.\ncluster autoscaler ensures there is always enough compute power to run your tasks, and that you aren’t paying extra for unused nodes.\nclusters may have periods where all batch processing jobs are complete, and the new batch doesn’t start until later in the day.\nEach autoscaler type is suitable in specific scenarios, so you should analyze the pros and cons of each to find the best choice.\nUsing a combination of all three types ensures that\nservices run stably at peak load times, and costs are minimized in times of lower demand. Deployment Strategies defines an application’s lifecycle that achieves and maintains the configured state for objects and applications in an automated manner. Effective deployment strategies minimize risk. Kubernetes deployment strategies are used to:\nDeploy, update, or rollback ReplicaSets, Pods, Services, and Applications Pause/Resume Deployments Scale Deployments manually or automatically There are are six types of deployment strategies:\nRecreate is simplest deployment strategy has short downtime between the shutdown of existing and deployment and the new deployment Pods running the live version of the application are all shut down simultaneously, and a new version of the application is deployed on newly created Pods. The rollback process is completed in reverse order. Pros Cons Simple setup Short downtime occurs between shutdown and new deployment Application version completely replaced - Rolling (ramped) each Pod is updated one at a time, single v1 Pod is replaced and is updated in this way until all Pods are v2. During rollback process, there is hardly any downtime since users are directed to either version. Pros Cons Simple setup Can’t control the traffic distribution Suitable for stateful applications that need to handle rebalancing of the data Rollout/rollback takes time Blue/green the blue environment is the live version of the application. the green environment is an exact copy that contains the deployment of the new version of the application. the green environment is created identical to the current production environment and is thoroughly tested. once all changes, bugs, and issues are addressed, user traffic is switched from the blue environment to the green environment. to perform a rollback, switch the environment back. Pros Cons Instance rollout/rollback (no downtime) Expensive(requires double resources) New version is available immediately to all users Rigorous testing required before releasing to production Handling stateful applications is difficult Canary the new version of the application is tested using a small set of random users alongside the current live version of the application. once the version of the application is successfully tested, it is then rolled out to all users. rollback has no downtime since few users are exposed to the new version. Pros Cons Convenient for reliability, error, and performance monitoring Slow rollout, gradual user access Fast rollback A/B testing known as split testing, evaluates two versions of an application (version A and version B). each version has features that cater to different sets of users. can select which version is best for global deployment based on user interaction and feedback. Pros Cons Multiple versions can run in parallel Requires intelligent load balancer Full control over traffic distribution Difficult to troubleshoot errors for a given session, distributed tracing becomes mandatory Shadow a “shadow version” of the application is deployed alongside the live version. user requests are sent to both versions, and both handle all requests, but the shadow version does not forward responses back to the users. lets developers see how the shadow version performs using real-world data without interrupting user experience. Pros Cons Performance testing with production traffic Expensive (double resources) No user impact Not a true user test, can lead to misinterpreted results No downtime Complex setup - Require monitoring for two environments Comparison Strategy Zero Downtime Real Traffic Testing Targeted Users Cloud Cost Rollback Duration Negative User Impact Complexity of Setup Recreate X X X •– ••• ••• - - - Ramped ✓ X X •– ••• •– •– Blue/Green ✓ X X ••• - - - ••- ••- Canary ✓ ✓ X •– •– •– ••- A/B Testing ✓ ✓ ✓ •– •– •– ••• Shadow ✓ ✓ X ••• - - - - - - ••• To create a good strategy:\nConsider the product type and the target audience Shadow and canary strategies use live user requests, as opposed to using a sample of users. The A/B testing strategy is useful if the version of the application requires minor tweaks or UI feature changes. The blue/green strategy is useful if your version of the application is complex or critical and needs proper monitoring with no downtime during deployment. The canary strategy is a good choice if you want zero downtime and are comfortable exposing your version of the application to the public. A rolling strategy gradually deploys the new version of the application. There is no downtime, and it is easy to roll back. The recreate strategy is a good choice if the application is not critical and users aren’t impacted by a short downtime. Rolling updates are automated updates that occur on a scheduled basis. They roll out automated and controlled app changes across pods, Work with pod templates like deployments, and allow for rollback as needed. To prepare your application to enable rolling updates,\nStep 1: Add liveness probes and readiness probes to deployments. That way deployments are appropriately marked as ‘ready.’ livenessProbe: httpGet: path: / port: 9080 initialDelaySeconds: 300 periodSeconds: 15 readinessProbe: httpGet: path: / port: 9080 initialDelaySeconds: 45 periodSeconds: 5 Step 2: add a rolling update strategy to the YAML file. apiVersion: apps/v1 kind: Deployment metadata: name: nginx-test spec: replicas: 10 # creating a deployment with 10 pods. selector: matchLabels: service: http-server # to wait a few seconds before moving to the next pod in the rollout stage minReadySeconds: 5 progressDeadlineSeconds: 600 strategy: type: RollingUpdate rollingUpdate: # strategy is to have at-least 50% of the pods always available # for a zero-downtime system, set the maxUnavailable to 0. # Setting the maxSurge to 100% would double the number of pods # and create a complete replica before taking the original set down # after the rollout is complete. maxUnavailable: 50% # there can only be 2 pods added to the 10 you defined earlier maxSurge: 2 Rolling out an application update Assume, You have a deployment with three pods in your ReplicaSet. Your application displays the message, “Hello world!” Your client has submitted a new request, and you have a new image for your application with a different message, “Hello world v2!’ to your users. // Older let port = process.env.PORT || 8080; let message = process.env.MESSAGE || 'Hello world!'; // Newer let port = process.env.PORT || 8080; let message = process.env.MESSAGE || 'Hello world v2!'; But you cannot have any downtime in your application. First, you need to build, tag, and upload this new image to Docker Hub. docker build -t hello-kubernetes . docker tag hello-kubernetes cham11ng/hello-kubernetes:2.0 docker push cham11ng/hello-kubernetes:2.0 # Your new software has been dockerized # and then updated to Docker Hub with the name # and tag cham11ng/hello-kubernetes:2.0”. Second, apply the new image to your deployment. kubectl get deployments # sets the image flag to the updated tag image on Docker Hub kubectl set image deployments/hello-kubernetes \\ hello-kubernetes=cham11ng/hello-kubernetes:2.0 deployment.extensions/hello-kubernetes image updated # observe that version 2 deployment has been rollout kubectl rollout status deployments/hello-kubernetes deployment \"hello-kubernetes\" successfully rolled out. Third, you can roll back your changes using the “rollout undo” command if there are errors in a deployment or the clients can change their minds. kubectl rollout undo deployments/hello-kubernetes deployment.extensions/hello-kubernetes rolled back kubectl get pods # we see three new pods that were created as port of the rollback # and can see some pods in terminating status How rolling works All-at-once In an all-at-once rollout, all v1 objects must be removed before v2 objects can become active.\nHere you see version 1 of an app with three pods running that users can access. When version 2 is deployed, new pods are created. The version 1 pods are marked for deletion and removed and user access is blocked. Once the version 1 pods are removed, the version 2 pods become active and user access is restored. Notice the time lag between deployment and pod updates. In an all-at-once rollback, all v2 objects must be removed before v1 objects can become active.\nHere you see version 2 of an app with three pods running that users can access. When version 1 of the app is deployed, new pods are created. The version 2 pods are marked for deletion and removed and user access is blocked. Once the version 2 pods are removed, the version 1 pods become active and user access is restored. One-at-a-time In a one-at-a-time rollout, the update is staggered so user access is not interrupted.\nHere you see version 1 of an app with three running pods that users can access. When version 2 is deployed, a new pod is created. The first version 1 pod is marked for deletion and removed, and the v2 pod becomes active. Now, a second v2 pod is created, and the second version 1 pod is marked for deletion and removed, and the second v2 pod becomes active. Then, a third v2 pod is created, and the third version 1 pod is marked for deletion and removed. And now the third v2 pod becomes active. In a one-at-a-time rollback, the update rollback is staggered so user access is not interrupted.\nHere you see version 2 of an app with three running pods that users can access. When version 1 of the app is deployed, a new pod is created. The first version 2 pod is marked for deletion and removed, and the v1 pod becomes active. Now, a second v1 pod is created. The second version 2 pod is marked for deletion and removed, and the second v1 pod becomes active. Then, a third v1 pod is created, and the third version 2 pod is marked for deletion and removed. And the third v1 pod becomes active. ConfigMaps helps developers avoid hard coding configuration variables in application code by keeping the configuration variables separate so that any changes in configuration settings do not require code changes. is an API object that stores non-confidential data in key-value pairs. provides configuration data to pods and deployments so that the configuration data is not hard coded inside the application code is meant for non-sensitive information as they do not provide secrecy or encryption. The data stored in a ConfigMap is limited and cannot exceed 1 megabyte For larger amounts of data, consider mounting a volume or use a separate database or file service has optional data and binaryData fields and no “spec\" field in the template the Config name must be a valid DNS subdomain name A ConfigMap is reusable for multiple deployments, thus decoupling the environment from the deployments themselves! Multiple ways to create a ConfigMap by using string literals, by using an existing “properties” or ”key” = “value” file, or by providing a ConfigMap YAML descriptor file. You can use the first and second ways to help create such a YAML file. Multiple ways to reference from pod/deployment to consume a ConfigMap reference by using environment variables with the configMapKeyRef attribute by mounting a file using the volumes plugin. (mount as volume) Kubernetes applies the ConfigMap to the pod or the deployment just before running the pod or deployment. Configuration: Environment Variable You’ll use the environment variable directly in the YAML file. Apply this deployment descriptor to our deployment. Here, the message is hard-coded in the descriptor file. --- spec: containers: - name: hello-kubernetes image: cham11ng/myapp:latest ports: - containerPort: 8080 env: # is used in the JavaScript file as process.env.MESSAGE - name: MESSAGE value: 'Hello from config file!' Configuration: ConfigMap string literal # provide a ConfigMap is to provide a key-value pair # in the create ConfigMap command. kubectl create ConfigMap my-config \\ --from-literal=MESSAGE=\"hello from first configmap\" After this first step, the second step is to tell our deployment about the new MESSAGE variable and specify its location for pickup.\nenv: - name: MESSAGE valueFrom: # to point to the ConfigMap created in the first step configMapKeyRef: # the deployment will look for a key named # MESSAGE in the ConfigMap named “my-config.” name: my-config key: MESSAGE Configuration: ConfigMap properties file Another way to add the MESSAGE variable in the ConfigMap is to use a file that contains all environment variables in the “key=value” format. Such a file is useful for adding many variables instead of listing those variables one by one on the command line. Here is a file with just one MESSAGE key and a value “hello from the my.properties file.” If you specify a directory to the “–from-file” flag, the entire directory is loaded into the ConfigMap. You can also load a specific file with a key by using the “–from-file=key=filename” format. kubectl create cm my-config --from-file=my.properties cat my.properties 1 MESSAGE=hello from my.properties file # to get the YAML output kubectl describe ConfigMap my-config env: - name: MESSAGE valueFrom: configMapKeyRef: name: my-config key: my.properties Configuration: ConfigMap YAML apiVersion: v1 data: my.properties: MESSAGE=hello from the my.properties file kind: ConfigMap metadata: name: my-config namespace: default kubectl get ConfigMap kubectl apply -f my-config.yaml kubectl describe cm my-config In our case, we have saved the output from ”kubectl get ConfigMap” as a YAML file called “my-config.yaml.” The first command indicates that there is no ConfigMap to begin with. Here you are creating the ConfigMap.yaml file. You’ll now apply the YAML file to your cluster which creates the ConfigMap. Note the MESSAGE in the ConfigMap file description. Secrets working with a Secret is like working with a ConfigMap Secret: Use with string literals # First, create a secret using a string literal. kubectl create secret generic api-creds \\ --from-literal=key=mysupersecretapikey # the get command verifies that the secret was created kubectl get secret # use the DESCRIBE command to verify our secret is indeed a secret # and check that you don’t see any secret, written using displayed text kubectl describe secret api-creds kubectl get secret api-creds -o YAML Secret: use with environment variables env: - name: API_CREDS # process.env.API_CREDS valueFrom: secretKeyRef: name: api-creds key: key Secret: use with volume mounts Each container in the descriptor file has its own volumeMount but shares the volume kubectl create secret generic api-creds \\ --from-literal=key=mysupersecretapikey spec: containers: - name: hello-kubernetes image: cham11ng/myapp:latest ports: - containerPort: 8080 # use a volume for the secret with a corresponding volumeMount volumeMounts: - name: api-creds # The api-creds secret is mounted as a file at /etc/api/api-creds mountPath: \"/etc/api\" readOnly: true volumes: - name: api-creds secrets: secretName: api-creds Service Binding is the process needed to consume external Services or backing Services, including REST APIs, databases, and event buses in our applications. manages configuration and credentials for back-end Services while protecting sensitive data. In addition, Service binding makes Service credentials available to you automatically as a Secret. consumes the external Service by binding the application to a deployment. Then, the application code uses the credentials from the binding and calls the corresponding Service. Here you can see an architectural diagram that illustrates the binding of a Kubernetes Cluster to an external Service. Next, let’s learn the steps required to bind the Service to your application. Architecture IBM Service binding Service binding quickly creates Service credentials for an IBM Cloud Service. You create the Service credentials using IBM’s public cloud Service endpoint and then store or “bind” your Service credentials in a Kubernetes Secret in your Cluster. Here’s how to bind an IBM Cloud Service to your Cluster: Provision an instance of the Service Bind the Service to your Cluster to create Service credentials for your Service that use the public cloud Service endpoint Store and retrieve the Service credentials in a Kubernetes Secret Configure your app to access the Service credentials in the Kubernetes Secret Commands Command Description kubectl autoscale deployment Autoscales a Kubernetes Deployment. kubectl create configmap Creates a ConfigMap resource. kubectl get deployments -o wide Lists deployments with details. kubectl get hpa Lists Horizontal Pod Autoscalers (hpa) kubectl scale deployment Scales a deployment. kubectl set image deployment Updates the current deployment. kubectl rollout Manages the rollout of a resource. kubectl rollout restart Restarts the resource so that the containers restart. kubectl rollout undo Rollbacks the resource. ","wordCount":"3783","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Agus Fahmi Aji Pramana"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/notes/kubernetes-201-managing-applications/"},"publisher":{"@type":"Organization","name":"Agus Fahmi Aji Pramana's Blogging Site","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Agus Fahmi Aji Pramana (Alt + H)">Agus Fahmi Aji Pramana</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=http://localhost:1313/about-me title="About Me"><span>About Me</span></a></li><li><a href=http://localhost:1313/blogs title=Blogs><span>Blogs</span></a></li><li><a href=http://localhost:1313/projects title=Projects><span>Projects</span></a></li><li><a href=http://localhost:1313/slides title=Slides><span>Slides</span></a></li><li><a href=http://localhost:1313/honors title=Honors><span>Honors</span></a></li><li><a href=http://localhost:1313/search title="Search (Alt + /)" accesskey=/><i class="fa fa-search"></i></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/notes/>Personal Notes, Learning, Self-Preparation</a></div><h1 class="post-title entry-hint-parent">Kubernetes 201: Managing Apps</h1><div class=post-description>Managing Applications with Kubernetes Notes</div><div class=post-meta>18 min&nbsp;·&nbsp;3783 words&nbsp;·&nbsp;Agus Fahmi Aji Pramana</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Agenda</span></summary><div class=inner><ul><li><a href=#replicaset aria-label=ReplicaSet>ReplicaSet</a><ul><li><a href=#create-replicaset-from-scratch aria-label="Create ReplicaSet from scratch">Create ReplicaSet from scratch</a></li></ul></li><li><a href=#autoscaling aria-label=Autoscaling>Autoscaling</a><ul><li><a href=#the-horizontal-pod-autoscaler-or-hpa aria-label="The Horizontal Pod Autoscaler (or HPA)">The Horizontal Pod Autoscaler (or HPA)</a></li><li><a href=#vertical-pod-autoscaler-or-vpa aria-label="Vertical Pod Autoscaler (or VPA)">Vertical Pod Autoscaler (or VPA)</a></li><li><a href=#cluster-autoscaler-or-ca aria-label="Cluster Autoscaler (or CA)">Cluster Autoscaler (or CA)</a></li></ul></li><li><a href=#deployment-strategies aria-label="Deployment Strategies">Deployment Strategies</a><ul><li><a href=#recreate aria-label=Recreate>Recreate</a></li><li><a href=#rolling-ramped aria-label="Rolling (ramped)">Rolling (ramped)</a></li><li><a href=#bluegreen aria-label=Blue/green>Blue/green</a></li><li><a href=#canary aria-label=Canary>Canary</a></li><li><a href=#ab-testing aria-label="A/B testing">A/B testing</a></li><li><a href=#shadow aria-label=Shadow>Shadow</a></li></ul></li><li><a href=#comparison aria-label=Comparison>Comparison</a></li><li><a href=#rolling-updates aria-label="Rolling updates">Rolling updates</a><ul><li><a href=#rolling-out-an-application-update aria-label="Rolling out an application update">Rolling out an application update</a></li><li><a href=#how-rolling-works aria-label="How rolling works">How rolling works</a><ul><li><a href=#all-at-once aria-label=All-at-once>All-at-once</a></li><li><a href=#one-at-a-time aria-label=One-at-a-time>One-at-a-time</a></li></ul></li></ul></li><li><a href=#configmaps aria-label=ConfigMaps>ConfigMaps</a><ul><li><a href=#configuration-environment-variable aria-label="Configuration: Environment Variable">Configuration: Environment Variable</a></li><li><a href=#configuration-configmap-string-literal aria-label="Configuration: ConfigMap string literal">Configuration: ConfigMap string literal</a></li><li><a href=#configuration-configmap-properties-file aria-label="Configuration: ConfigMap properties file">Configuration: ConfigMap properties file</a></li><li><a href=#configuration-configmap-yaml aria-label="Configuration: ConfigMap YAML">Configuration: ConfigMap YAML</a></li></ul></li><li><a href=#secrets aria-label=Secrets>Secrets</a><ul><li><a href=#secret-use-with-string-literals aria-label="Secret: Use with string literals">Secret: Use with string literals</a></li><li><a href=#secret-use-with-environment-variables aria-label="Secret: use with environment variables">Secret: use with environment variables</a></li><li><a href=#secret-use-with-volume-mounts aria-label="Secret: use with volume mounts">Secret: use with volume mounts</a></li></ul></li><li><a href=#service-binding aria-label="Service Binding">Service Binding</a><ul><li><a href=#architecture aria-label=Architecture>Architecture</a></li><li><a href=#ibm-service-binding aria-label="IBM Service binding">IBM Service binding</a></li></ul></li><li><a href=#commands aria-label=Commands>Commands</a></li></ul></div></details></div><div class=post-content><h2 id=replicaset>ReplicaSet<a hidden class=anchor aria-hidden=true href=#replicaset>#</a></h2><ul><li>ensures the right number of pods are always up and running</li><li>provide high availability through redundancy</li><li>adds or deletes pods for scaling</li><li>always tries to match the actual state of the replicas to the desired state</li><li>replaces failing pods or deletes additional pods to maintain the desired state</li><li>supersedes ReplicaControllers</li><li>Deployments manage ReplicaSets, send pods declarative updates</li></ul><h3 id=create-replicaset-from-scratch>Create ReplicaSet from scratch<a hidden class=anchor aria-hidden=true href=#create-replicaset-from-scratch>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=c># replicaset.yaml</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>apps/v1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>ReplicaSet</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>hello-kubernetes</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>replicas</span><span class=p>:</span><span class=w> </span><span class=m>1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>selector</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>matchLabels</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>hello-kubernetes</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>template</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>hello-kubernetes</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>containers</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>hello-kubernetes</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>paulbouwer/hello-kubernets:1.5</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span><span class=nt>-containerPort</span><span class=p>:</span><span class=w> </span><span class=m>8080</span><span class=w>
</span></span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Outputs ReplicaSet was created</span>
</span></span><span class=line><span class=cl>kubectl create -f replicaset.yaml
</span></span><span class=line><span class=cl><span class=c1># Confirm by using &#34;get pods&#34;</span>
</span></span><span class=line><span class=cl>kubectl get pods
</span></span><span class=line><span class=cl><span class=c1># Output shows details about ReplicaSet and pod created</span>
</span></span><span class=line><span class=cl>kubectl get rs
</span></span></code></pre></div><blockquote><p>Note: Creating a Deployment that includes a ReplicaSet is recommended over creating a standalone ReplicaSet.</p></blockquote><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Create deployment</span>
</span></span><span class=line><span class=cl>kubectl create -f deployment.yaml
</span></span><span class=line><span class=cl>kubectl get pods
</span></span><span class=line><span class=cl>kubectl get deploy
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Scale deployment</span>
</span></span><span class=line><span class=cl>kubectl scale deploy hello-kubernetes --replicas<span class=o>=</span><span class=m>3</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Should shows 3 pods running</span>
</span></span><span class=line><span class=cl>kubectl get pods
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Delete pod</span>
</span></span><span class=line><span class=cl>kubectl delete pod hello-kubernetes-2324343-5mflw
</span></span><span class=line><span class=cl><span class=c1># Here desired state (--replicas=3) does not</span>
</span></span><span class=line><span class=cl><span class=c1># match the actual state [running pods=2]</span>
</span></span><span class=line><span class=cl><span class=c1># and deleted pod replaced by new one</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Create pod extra pod</span>
</span></span><span class=line><span class=cl>kubectl create pod hello-kubernetes-122323
</span></span><span class=line><span class=cl><span class=c1># Here desired state (--replicas=3) does not</span>
</span></span><span class=line><span class=cl><span class=c1># match the actual state [running pods=4]</span>
</span></span><span class=line><span class=cl><span class=c1># and new pod is marked for deletion and removed automatically</span>
</span></span></code></pre></div><h2 id=autoscaling>Autoscaling<a hidden class=anchor aria-hidden=true href=#autoscaling>#</a></h2><ul><li>ReplicaSets provide a good start for scaling, but you don’t always want 10 instances of your resource running.</li><li>Kubernetes autoscaling helps optimize resource usage and costs by automatically scaling a cluster in line with demand as needed.</li><li>Kubernetes enables autoscaling at two different layers:<ul><li>the cluster or node level</li><li>the pod level.</li></ul></li><li>Three types of autoscalers are available in Kubernetes:<ul><li>The Horizontal Pod Autoscaler (or HPA)</li><li>Vertical Pod Autoscaler (or VPA)</li><li>Cluster Autoscaler (or CA)</li></ul></li></ul><p>To create autoscaling:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl get pods
</span></span><span class=line><span class=cl>kubectl get rs
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>kubectl autoscale deploy hello-kubernetes --min<span class=o>=</span><span class=m>2</span> --max<span class=o>=</span><span class=m>5</span> --cpu-percent<span class=o>=</span><span class=m>50</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>kubectl describe rs hello-kubernetes-&lt;hash&gt;
</span></span></code></pre></div><ul><li>List the current number or state of pods.</li><li>A ReplicaSet is automatically created when you create a deployment. In order to autoscale, you simply use the autoscale command with the requisite attributes.</li><li>Min is the number of minimum pods – notice that we have changed the value of “Min” to 2.</li><li>Max is the number of maximum pods.</li><li>CPU-percent acts as a trigger that tells the system to create a new pod when the CPU usage reaches 50% across the cluster.</li><li>In the background, the deployment still uses the ReplicaSet to scale up and down.</li><li>The describe command shows the number of replicas in the &ldquo;autoscaled&rdquo; ReplicaSet.</li></ul><h3 id=the-horizontal-pod-autoscaler-or-hpa>The Horizontal Pod Autoscaler (or HPA)<a hidden class=anchor aria-hidden=true href=#the-horizontal-pod-autoscaler-or-hpa>#</a></h3><ul><li><strong>adjusts</strong> the number of replicas of an application by increasing or decreasing the number of pods.</li><li>automatically updates a workload resource (like a deployment) by horizontally scaling the workload to match the demand</li><li>Horizontal scaling, or “scaling out,” automatically increases or decreases the number of running pods as application usage changes.</li><li>uses a cluster operator that sets targets for metrics like CPU or memory utilization and the maximum and minimum desired number of replicas</li><li>even though you can create an HPA autoscaler from scratch, you should use the autoscale command instead.</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl get hpa
</span></span></code></pre></div><p>For example,</p><p><input type=checkbox id=zoomCheck-fe0a1 hidden>
<label for=zoomCheck-fe0a1><img class=zoomCheck loading=lazy decoding=async src=img/hpa-graph.webp alt="HPA Graph"></label></p><ul><li>the system load is low early in the morning, so one pod is sufficient. The HPA autoscales the workload resource to meet usage demand.</li><li>By 11am, peak load drives a need for three pods, so the HPA autoscales the workload resource to meet usage demand.</li><li>Usage drops in the afternoon, so the third pod is marked for deletion and removed.</li><li>And usage drops even lower by 5pm, so another pod is marked for deletion and removed.</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=c># another way to enable autoscaling is to manually</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c># create the HPA object from a YAML file.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>autoscaling/v1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>HorizontalPodAutoScaler</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>hello-kubernetes</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>default</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w> </span><span class=c># can set the minimum and maximum number of pods.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>maxReplicas</span><span class=p>:</span><span class=w> </span><span class=m>5</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>minReplicas</span><span class=p>:</span><span class=w> </span><span class=m>2</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>scaleTargetRef</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>apps/v1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Deployment</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>hello-kubernetes</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=c># The CPU-percent flag shows up as “targetCPUUtilizationPercentage”.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>targetCPUUtilizationPercentage</span><span class=p>:</span><span class=w> </span><span class=m>10</span><span class=w>
</span></span></span></code></pre></div><h3 id=vertical-pod-autoscaler-or-vpa>Vertical Pod Autoscaler (or VPA)<a hidden class=anchor aria-hidden=true href=#vertical-pod-autoscaler-or-vpa>#</a></h3><ul><li><strong>adjusts</strong> the resource requests and limits of a container by increasing or decreasing the resource size or speed of the pods.</li><li>A best practice is to scale horizontally, but there are some services you may want to run in a cluster where horizontal scaling is impossible or not ideal.</li><li>Vertical scaling, or “scaling up,” refers to adding more resources to an existing machine.</li><li>lets you scale a service vertically within a cluster.</li><li>uses a cluster operator sets targets for metrics like CPU or memory utilization, similar to an HPA.</li><li>The cluster then reconciles the size of the service’s pod or pods based on their current usage and the desired target.</li></ul><p>For example,</p><p><input type=checkbox id=zoomCheck-074d3 hidden>
<label for=zoomCheck-074d3><img class=zoomCheck loading=lazy decoding=async src=img/vpa-graph.webp alt="VPA Graph"></label></p><ul><li>the system load is low early in the morning, so system resources used by the pod are low.</li><li>By 11am, peak load drives a need for more capacity.</li><li>The VPA autoscales the pod by adding more system resources (CPU and memory) to meet the demand.</li><li>Usage drops in the afternoon, so the pod is autoscaled to use fewer system resources.</li><li>And usage drops even lower by 5pm, so the pod is autoscaled further to match the 7am levels.</li></ul><blockquote><p><strong>Note:</strong> You should not use VPAs with HPAs on resource metrics like CPU or memory. However, you can use them together on custom or external metrics.</p></blockquote><h3 id=cluster-autoscaler-or-ca>Cluster Autoscaler (or CA)<a hidden class=anchor aria-hidden=true href=#cluster-autoscaler-or-ca>#</a></h3><ul><li><strong>adjusts</strong> the number of nodes in the cluster when pods fail to schedule, or demand increases or decreases in relation to the existing nodes’ capacity</li><li>autoscales the cluster itself, increasing and decreasing the number of available nodes that pods can run on.</li><li>Pods are autoscaled using HPA or VPA, but when the nodes themselves are overloaded with pods.</li><li>can use a CA to autoscale the nodes so that the pods can rebalance themselves across the cluster.</li></ul><p>For example,</p><p><input type=checkbox id=zoomCheck-49adf hidden>
<label for=zoomCheck-49adf><img class=zoomCheck loading=lazy decoding=async src=img/ca-graph.webp alt="CA Graph"></label></p><ul><li><p>the system load is low early in the morning, so existing nodes can handle the load.</p></li><li><p>When demand increases, new pod requests come in, and the CA autoscales the cluster by adding a new node and pods to meet the demand.</p></li><li><p>By 11 am, peak load brings the new node to full capacity.</p></li><li><p>When usage drops in the afternoon, unused pods are marked for deletion and removed.</p></li><li><p>And when usage drops even lower by 5 pm, all pods in the new node are marked for deletion and removed.</p></li><li><p>And then the node itself is marked and removed.</p></li><li><p>cluster autoscaler ensures there is always enough compute power to run your tasks, and that you aren’t paying extra for unused nodes.</p></li><li><p>clusters may have periods where all batch processing jobs are complete, and the new batch doesn’t start until later in the day.</p></li><li><p>Each autoscaler type is suitable in specific scenarios, so you should analyze the pros and cons of each to find the best choice.</p></li><li><p>Using a combination of all three types ensures that</p><ul><li>services run stably at peak load times, and</li><li>costs are minimized in times of lower demand.</li></ul></li></ul><h2 id=deployment-strategies>Deployment Strategies<a hidden class=anchor aria-hidden=true href=#deployment-strategies>#</a></h2><ul><li>defines an application’s lifecycle that achieves and maintains the configured state for objects and applications in an automated manner. Effective deployment strategies minimize risk.</li></ul><p>Kubernetes deployment strategies are used to:</p><ul><li>Deploy, update, or rollback ReplicaSets, Pods, Services, and Applications</li><li>Pause/Resume Deployments</li><li>Scale Deployments manually or automatically</li></ul><p>There are are six types of deployment strategies:</p><h3 id=recreate>Recreate<a hidden class=anchor aria-hidden=true href=#recreate>#</a></h3><ul><li>is simplest deployment strategy</li><li>has short downtime between the shutdown of existing and deployment and the new deployment</li><li>Pods running the live version of the application are <strong>all shut down simultaneously</strong>, and a new version of the application is deployed on newly created Pods.</li><li>The rollback process is completed in reverse order.</li></ul><table><thead><tr><th>Pros</th><th>Cons</th></tr></thead><tbody><tr><td>Simple setup</td><td>Short downtime occurs between shutdown and new deployment</td></tr><tr><td>Application version completely replaced</td><td>-</td></tr></tbody></table><p><input type=checkbox id=zoomCheck-b8326 hidden>
<label for=zoomCheck-b8326><img class=zoomCheck loading=lazy decoding=async src=img/recreate-strategy.webp alt="Recreate Strategy"></label></p><h3 id=rolling-ramped>Rolling (ramped)<a hidden class=anchor aria-hidden=true href=#rolling-ramped>#</a></h3><ul><li>each Pod is updated one at a time, single v1 Pod is replaced and is updated in this way until all Pods are v2.</li><li>During rollback process, there is hardly any downtime since users are directed to either version.</li></ul><table><thead><tr><th>Pros</th><th>Cons</th></tr></thead><tbody><tr><td>Simple setup</td><td>Can&rsquo;t control the traffic distribution</td></tr><tr><td>Suitable for stateful applications that need to handle rebalancing of the data</td><td>Rollout/rollback takes time</td></tr></tbody></table><p><input type=checkbox id=zoomCheck-ac62e hidden>
<label for=zoomCheck-ac62e><img class=zoomCheck loading=lazy decoding=async src=img/ramped-stretegy.webp alt="Ramped Strategy"></label></p><h3 id=bluegreen>Blue/green<a hidden class=anchor aria-hidden=true href=#bluegreen>#</a></h3><ul><li>the blue environment is the live version of the application.</li><li>the green environment is an exact copy that contains the deployment of the new version of the application.</li><li>the green environment is created identical to the current production environment and is thoroughly tested.</li><li>once all changes, bugs, and issues are addressed, user traffic is switched from the blue environment to the green environment.</li><li>to perform a rollback, switch the environment back.</li></ul><table><thead><tr><th>Pros</th><th>Cons</th></tr></thead><tbody><tr><td>Instance rollout/rollback (no downtime)</td><td>Expensive(requires double resources)</td></tr><tr><td>New version is available immediately to all users</td><td>Rigorous testing required before releasing to production</td></tr><tr><td></td><td>Handling stateful applications is difficult</td></tr></tbody></table><p><input type=checkbox id=zoomCheck-2aede hidden>
<label for=zoomCheck-2aede><img class=zoomCheck loading=lazy decoding=async src=img/blue-green-deployment-strategy.webp alt="Blue and green Deployment Strategy"></label></p><h3 id=canary>Canary<a hidden class=anchor aria-hidden=true href=#canary>#</a></h3><ul><li>the new version of the application is tested using a small set of random users alongside the current live version of the application.</li><li>once the version of the application is successfully tested, it is then rolled out to all users.</li><li>rollback has no downtime since few users are exposed to the new version.</li></ul><table><thead><tr><th>Pros</th><th>Cons</th></tr></thead><tbody><tr><td>Convenient for reliability, error, and performance monitoring</td><td>Slow rollout, gradual user access</td></tr><tr><td>Fast rollback</td><td></td></tr></tbody></table><p><input type=checkbox id=zoomCheck-5718c hidden>
<label for=zoomCheck-5718c><img class=zoomCheck loading=lazy decoding=async src=img/canary-strategy.webp alt="Canary Strategy"></label></p><h3 id=ab-testing>A/B testing<a hidden class=anchor aria-hidden=true href=#ab-testing>#</a></h3><ul><li>known as split testing, evaluates two versions of an application (version A and version B).</li><li>each version has features that cater to different sets of users.</li><li>can select which version is best for global deployment based on user interaction and feedback.</li></ul><table><thead><tr><th>Pros</th><th>Cons</th></tr></thead><tbody><tr><td>Multiple versions can run in parallel</td><td>Requires intelligent load balancer</td></tr><tr><td>Full control over traffic distribution</td><td>Difficult to troubleshoot errors for a given session, distributed tracing becomes mandatory</td></tr></tbody></table><p><input type=checkbox id=zoomCheck-a7786 hidden>
<label for=zoomCheck-a7786><img class=zoomCheck loading=lazy decoding=async src=img/a-b-testing-strategy.webp alt="A/B Testing Strategy"></label></p><h3 id=shadow>Shadow<a hidden class=anchor aria-hidden=true href=#shadow>#</a></h3><ul><li>a &ldquo;shadow version&rdquo; of the application is deployed alongside the live version.</li><li>user requests are sent to both versions, and both handle all requests, but the shadow version does not forward responses back to the users.</li><li>lets developers see how the shadow version performs using real-world data without interrupting user experience.</li></ul><table><thead><tr><th>Pros</th><th>Cons</th></tr></thead><tbody><tr><td>Performance testing with production traffic</td><td>Expensive (double resources)</td></tr><tr><td>No user impact</td><td>Not a true user test, can lead to misinterpreted results</td></tr><tr><td>No downtime</td><td>Complex setup</td></tr><tr><td>-</td><td>Require monitoring for two environments</td></tr></tbody></table><p><input type=checkbox id=zoomCheck-a4058 hidden>
<label for=zoomCheck-a4058><img class=zoomCheck loading=lazy decoding=async src=img/shadow-strategy.webp alt="Shadow Strategy"></label></p><h2 id=comparison>Comparison<a hidden class=anchor aria-hidden=true href=#comparison>#</a></h2><table><thead><tr><th>Strategy</th><th>Zero Downtime</th><th>Real Traffic Testing</th><th>Targeted Users</th><th>Cloud Cost</th><th>Rollback Duration</th><th>Negative User Impact</th><th>Complexity of Setup</th></tr></thead><tbody><tr><td>Recreate</td><td>X</td><td>X</td><td>X</td><td>•&ndash;</td><td>•••</td><td>•••</td><td>- - -</td></tr><tr><td>Ramped</td><td>✓</td><td>X</td><td>X</td><td>•&ndash;</td><td>•••</td><td>•&ndash;</td><td>•&ndash;</td></tr><tr><td>Blue/Green</td><td>✓</td><td>X</td><td>X</td><td>•••</td><td>- - -</td><td>••-</td><td>••-</td></tr><tr><td>Canary</td><td>✓</td><td>✓</td><td>X</td><td>•&ndash;</td><td>•&ndash;</td><td>•&ndash;</td><td>••-</td></tr><tr><td>A/B Testing</td><td>✓</td><td>✓</td><td>✓</td><td>•&ndash;</td><td>•&ndash;</td><td>•&ndash;</td><td>•••</td></tr><tr><td>Shadow</td><td>✓</td><td>✓</td><td>X</td><td>•••</td><td>- - -</td><td>- - -</td><td>•••</td></tr></tbody></table><p>To create a good strategy:</p><ul><li>Consider the product type and the target audience</li><li>Shadow and canary strategies use live user requests, as opposed to using a sample of users.</li><li>The A/B testing strategy is useful if the version of the application requires minor tweaks or UI feature changes.</li><li>The blue/green strategy is useful if your version of the application is complex or critical and needs proper monitoring with no downtime during deployment.</li><li>The canary strategy is a good choice if you want zero downtime and are comfortable exposing your version of the application to the public.</li><li>A rolling strategy gradually deploys the new version of the application. There is no downtime, and it is easy to roll back.</li><li>The recreate strategy is a good choice if the application is not critical and users aren’t impacted by a short downtime.</li></ul><h2 id=rolling-updates>Rolling updates<a hidden class=anchor aria-hidden=true href=#rolling-updates>#</a></h2><ul><li>are automated updates that occur on a scheduled basis.<ul><li>They roll out automated and controlled app changes across pods,</li><li>Work with pod templates like deployments, and</li><li>allow for rollback as needed.</li></ul></li></ul><p>To prepare your application to enable rolling updates,</p><ul><li>Step 1: Add liveness probes and readiness probes to deployments. That way deployments are appropriately marked as ‘ready.’</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>livenessProbe:
</span></span><span class=line><span class=cl>  httpGet:
</span></span><span class=line><span class=cl>    path: /
</span></span><span class=line><span class=cl>    port: <span class=m>9080</span>
</span></span><span class=line><span class=cl>  initialDelaySeconds: <span class=m>300</span>
</span></span><span class=line><span class=cl>  periodSeconds: <span class=m>15</span>
</span></span><span class=line><span class=cl>readinessProbe:
</span></span><span class=line><span class=cl>  httpGet:
</span></span><span class=line><span class=cl>    path: /
</span></span><span class=line><span class=cl>    port: <span class=m>9080</span>
</span></span><span class=line><span class=cl>  initialDelaySeconds: <span class=m>45</span>
</span></span><span class=line><span class=cl>  periodSeconds: <span class=m>5</span>
</span></span></code></pre></div><ul><li>Step 2: add a rolling update strategy to the YAML file.</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>apiVersion: apps/v1
</span></span><span class=line><span class=cl>kind: Deployment
</span></span><span class=line><span class=cl>metadata:
</span></span><span class=line><span class=cl>  name: nginx-test
</span></span><span class=line><span class=cl>spec:
</span></span><span class=line><span class=cl>  replicas: <span class=m>10</span> <span class=c1># creating a deployment with 10 pods.</span>
</span></span><span class=line><span class=cl>  selector:
</span></span><span class=line><span class=cl>    matchLabels:
</span></span><span class=line><span class=cl>      service: http-server
</span></span><span class=line><span class=cl>  <span class=c1># to wait a few seconds before moving to the next pod in the rollout stage</span>
</span></span><span class=line><span class=cl>  minReadySeconds: <span class=m>5</span>
</span></span><span class=line><span class=cl>  progressDeadlineSeconds: <span class=m>600</span>
</span></span><span class=line><span class=cl>  strategy:
</span></span><span class=line><span class=cl>    type: RollingUpdate
</span></span><span class=line><span class=cl>    rollingUpdate:
</span></span><span class=line><span class=cl>      <span class=c1># strategy is to have at-least 50% of the pods always available</span>
</span></span><span class=line><span class=cl>      <span class=c1># for a zero-downtime system, set the maxUnavailable to 0.</span>
</span></span><span class=line><span class=cl>      <span class=c1># Setting the maxSurge to 100% would double the number of pods</span>
</span></span><span class=line><span class=cl>      <span class=c1># and create a complete replica before taking the original set down</span>
</span></span><span class=line><span class=cl>      <span class=c1># after the rollout is complete.</span>
</span></span><span class=line><span class=cl>      maxUnavailable: 50%
</span></span><span class=line><span class=cl>      <span class=c1># there can only be 2 pods added to the 10 you defined earlier</span>
</span></span><span class=line><span class=cl>      maxSurge: <span class=m>2</span>
</span></span></code></pre></div><h3 id=rolling-out-an-application-update>Rolling out an application update<a hidden class=anchor aria-hidden=true href=#rolling-out-an-application-update>#</a></h3><ul><li>Assume, You have a deployment with three pods in your ReplicaSet.</li><li>Your application displays the message, “Hello world!”</li><li>Your client has submitted a new request, and you have a new image for your application with a different message, “Hello world v2!’ to your users.</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-js data-lang=js><span class=line><span class=cl><span class=c1>// Older
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=kd>let</span> <span class=nx>port</span> <span class=o>=</span> <span class=nx>process</span><span class=p>.</span><span class=nx>env</span><span class=p>.</span><span class=nx>PORT</span> <span class=o>||</span> <span class=mi>8080</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=kd>let</span> <span class=nx>message</span> <span class=o>=</span> <span class=nx>process</span><span class=p>.</span><span class=nx>env</span><span class=p>.</span><span class=nx>MESSAGE</span> <span class=o>||</span> <span class=s1>&#39;Hello world!&#39;</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=c1>// Newer
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=kd>let</span> <span class=nx>port</span> <span class=o>=</span> <span class=nx>process</span><span class=p>.</span><span class=nx>env</span><span class=p>.</span><span class=nx>PORT</span> <span class=o>||</span> <span class=mi>8080</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=kd>let</span> <span class=nx>message</span> <span class=o>=</span> <span class=nx>process</span><span class=p>.</span><span class=nx>env</span><span class=p>.</span><span class=nx>MESSAGE</span> <span class=o>||</span> <span class=s1>&#39;Hello world v2!&#39;</span><span class=p>;</span>
</span></span></code></pre></div><ul><li>But you cannot have any downtime in your application.</li><li>First, you need to build, tag, and upload this new image to Docker Hub.</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>docker build -t hello-kubernetes .
</span></span><span class=line><span class=cl>docker tag hello-kubernetes cham11ng/hello-kubernetes:2.0
</span></span><span class=line><span class=cl>docker push cham11ng/hello-kubernetes:2.0
</span></span><span class=line><span class=cl><span class=c1># Your new software has been dockerized</span>
</span></span><span class=line><span class=cl><span class=c1># and then updated to Docker Hub with the name</span>
</span></span><span class=line><span class=cl><span class=c1># and tag cham11ng/hello-kubernetes:2.0”.</span>
</span></span></code></pre></div><ul><li>Second, apply the new image to your deployment.</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl get deployments
</span></span><span class=line><span class=cl><span class=c1># sets the image flag to the updated tag image on Docker Hub</span>
</span></span><span class=line><span class=cl>kubectl <span class=nb>set</span> image deployments/hello-kubernetes <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  hello-kubernetes<span class=o>=</span>cham11ng/hello-kubernetes:2.0
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  deployment.extensions/hello-kubernetes image updated
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># observe that version 2 deployment has been rollout</span>
</span></span><span class=line><span class=cl>kubectl rollout status deployments/hello-kubernetes
</span></span><span class=line><span class=cl>  deployment <span class=s2>&#34;hello-kubernetes&#34;</span> successfully rolled out.
</span></span></code></pre></div><ul><li>Third, you can roll back your changes using the &ldquo;rollout undo&rdquo; command if there are errors in a deployment or the clients can change their minds.</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl rollout undo deployments/hello-kubernetes
</span></span><span class=line><span class=cl>  deployment.extensions/hello-kubernetes rolled back
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>kubectl get pods
</span></span><span class=line><span class=cl><span class=c1># we see three new pods that were created as port of the rollback</span>
</span></span><span class=line><span class=cl><span class=c1># and can see some pods in terminating status</span>
</span></span></code></pre></div><h3 id=how-rolling-works>How rolling works<a hidden class=anchor aria-hidden=true href=#how-rolling-works>#</a></h3><h4 id=all-at-once>All-at-once<a hidden class=anchor aria-hidden=true href=#all-at-once>#</a></h4><p>In an <strong>all-at-once rollout</strong>, all v1 objects must be removed before v2 objects can become active.</p><p><input type=checkbox id=zoomCheck-564b5 hidden>
<label for=zoomCheck-564b5><img class=zoomCheck loading=lazy decoding=async src=img/all-at-once-rollout.webp alt="All-at-once rollout"></label></p><ul><li>Here you see version 1 of an app with three pods running that users can access.</li><li>When version 2 is deployed, new pods are created.</li><li>The version 1 pods are marked for deletion and removed and user access is blocked.</li><li>Once the version 1 pods are removed, the version 2 pods become active and user access is restored.</li><li>Notice the time lag between deployment and pod updates.</li></ul><p>In an <strong>all-at-once rollback</strong>, all v2 objects must be removed before v1 objects can become active.</p><p><input type=checkbox id=zoomCheck-e00f0 hidden>
<label for=zoomCheck-e00f0><img class=zoomCheck loading=lazy decoding=async src=img/all-at-once-rollback.webp alt="All-at-once rollback"></label></p><ul><li>Here you see version 2 of an app with three pods running that users can access.</li><li>When version 1 of the app is deployed, new pods are created.</li><li>The version 2 pods are marked for deletion and removed and user access is blocked.</li><li>Once the version 2 pods are removed, the version 1 pods become active and user access is restored.</li></ul><h4 id=one-at-a-time>One-at-a-time<a hidden class=anchor aria-hidden=true href=#one-at-a-time>#</a></h4><p>In a <strong>one-at-a-time rollout</strong>, the update is staggered so user access is not interrupted.</p><p><input type=checkbox id=zoomCheck-ffaac hidden>
<label for=zoomCheck-ffaac><img class=zoomCheck loading=lazy decoding=async src=img/one-at-a-time-rollout.webp alt="One-at-a-time rollout"></label></p><ul><li>Here you see version 1 of an app with three running pods that users can access.</li><li>When version 2 is deployed, a new pod is created.</li><li>The first version 1 pod is marked for deletion and removed, and the v2 pod becomes active.</li><li>Now, a second v2 pod is created, and the second version 1 pod is marked for deletion and removed, and the second v2 pod becomes active.</li><li>Then, a third v2 pod is created, and the third version 1 pod is marked for deletion and removed. And now the third v2 pod becomes active.</li></ul><p>In a <strong>one-at-a-time rollback</strong>, the update rollback is staggered so user access is not interrupted.</p><p><input type=checkbox id=zoomCheck-8f727 hidden>
<label for=zoomCheck-8f727><img class=zoomCheck loading=lazy decoding=async src=img/one-at-a-time-rollback.webp alt="One-at-a-time rollback"></label></p><ul><li>Here you see version 2 of an app with three running pods that users can access.</li><li>When version 1 of the app is deployed, a new pod is created.</li><li>The first version 2 pod is marked for deletion and removed, and the v1 pod becomes active.</li><li>Now, a second v1 pod is created. The second version 2 pod is marked for deletion and removed, and the second v1 pod becomes active.</li><li>Then, a third v1 pod is created, and the third version 2 pod is marked for deletion and removed. And the third v1 pod becomes active.</li></ul><h2 id=configmaps>ConfigMaps<a hidden class=anchor aria-hidden=true href=#configmaps>#</a></h2><ul><li>helps developers avoid hard coding configuration variables in application code by keeping the configuration variables separate so that any changes in configuration settings do not require code changes.</li><li>is an API object that stores non-confidential data in key-value pairs.</li><li>provides configuration data to pods and deployments so that the configuration data is not hard coded inside the application code</li><li>is meant for non-sensitive information as they do not provide secrecy or encryption.</li><li>The data stored in a ConfigMap is limited and cannot exceed 1 megabyte<ul><li>For larger amounts of data, consider mounting a volume or use a separate database or file service</li></ul></li><li>has optional data and binaryData fields and no “spec" field in the template</li><li>the Config name must be a valid DNS subdomain name</li><li>A ConfigMap is reusable for multiple deployments, thus decoupling the environment from the deployments themselves!</li><li>Multiple ways to create<ul><li>a ConfigMap by using string literals,</li><li>by using an existing “properties” or ”key” = “value” file,</li><li>or by providing a ConfigMap YAML descriptor file. You can use the first and second ways to help create such a YAML file.</li></ul></li><li>Multiple ways to reference from pod/deployment to consume a ConfigMap<ul><li>reference by using environment variables with the configMapKeyRef attribute</li><li>by mounting a file using the volumes plugin. (mount as volume)</li></ul></li><li>Kubernetes applies the ConfigMap to the pod or the deployment just before running the pod or deployment.</li></ul><h3 id=configuration-environment-variable>Configuration: Environment Variable<a hidden class=anchor aria-hidden=true href=#configuration-environment-variable>#</a></h3><ul><li>You’ll use the environment variable directly in the YAML file.</li><li>Apply this deployment descriptor to our deployment.</li><li>Here, the message is hard-coded in the descriptor file.</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nn>---</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>containers</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>hello-kubernetes</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>cham11ng/myapp:latest</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span>- <span class=nt>containerPort</span><span class=p>:</span><span class=w> </span><span class=m>8080</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>env</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=c># is used in the JavaScript file as process.env.MESSAGE</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>MESSAGE</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>value</span><span class=p>:</span><span class=w> </span><span class=s1>&#39;Hello from config file!&#39;</span><span class=w>
</span></span></span></code></pre></div><h3 id=configuration-configmap-string-literal>Configuration: ConfigMap string literal<a hidden class=anchor aria-hidden=true href=#configuration-configmap-string-literal>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># provide a ConfigMap is to provide a key-value pair</span>
</span></span><span class=line><span class=cl><span class=c1># in the create ConfigMap command.</span>
</span></span><span class=line><span class=cl>kubectl create ConfigMap my-config <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --from-literal<span class=o>=</span><span class=nv>MESSAGE</span><span class=o>=</span><span class=s2>&#34;hello from first configmap&#34;</span>
</span></span></code></pre></div><p>After this first step, the second step is to tell our deployment about the new MESSAGE variable and specify its location for pickup.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>env</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>MESSAGE</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>valueFrom</span><span class=p>:</span><span class=w> </span><span class=c># to point to the ConfigMap created in the first step</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>configMapKeyRef</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=c># the deployment will look for a key named</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=c># MESSAGE in the ConfigMap named “my-config.”</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>my-config</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>key</span><span class=p>:</span><span class=w> </span><span class=l>MESSAGE</span><span class=w>
</span></span></span></code></pre></div><h3 id=configuration-configmap-properties-file>Configuration: ConfigMap properties file<a hidden class=anchor aria-hidden=true href=#configuration-configmap-properties-file>#</a></h3><ul><li>Another way to add the MESSAGE variable in the ConfigMap is to use a file that contains all environment variables in the “key=value” format.</li><li>Such a file is useful for adding many variables instead of listing those variables one by one on the command line.</li><li>Here is a file with just one MESSAGE key and a value “hello from the my.properties file.”</li><li>If you specify a directory to the “&ndash;from-file” flag, the entire directory is loaded into the ConfigMap.</li><li>You can also load a specific file with a key by using the “&ndash;from-file=key=filename” format.</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl create cm my-config --from-file<span class=o>=</span>my.properties
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>cat my.properties
</span></span><span class=line><span class=cl>  <span class=m>1</span> <span class=nv>MESSAGE</span><span class=o>=</span>hello from my.properties file
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># to get the YAML output</span>
</span></span><span class=line><span class=cl>kubectl describe ConfigMap my-config
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>env</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>MESSAGE</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>valueFrom</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>configMapKeyRef</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>my-config</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>key</span><span class=p>:</span><span class=w> </span><span class=l>my.properties</span><span class=w>
</span></span></span></code></pre></div><h3 id=configuration-configmap-yaml>Configuration: ConfigMap YAML<a hidden class=anchor aria-hidden=true href=#configuration-configmap-yaml>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>data</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>my.properties</span><span class=p>:</span><span class=w> </span><span class=l>MESSAGE=hello from the my.properties file</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>ConfigMap</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>my-config</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>default</span><span class=w>
</span></span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl get ConfigMap
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>kubectl apply -f my-config.yaml
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>kubectl describe cm my-config
</span></span></code></pre></div><ul><li>In our case, we have saved the output from ”kubectl get ConfigMap” as a YAML file called “my-config.yaml.”</li><li>The first command indicates that there is no ConfigMap to begin with. Here you are creating the ConfigMap.yaml file.</li><li>You’ll now apply the YAML file to your cluster which creates the ConfigMap.</li><li>Note the MESSAGE in the ConfigMap file description.</li></ul><h2 id=secrets>Secrets<a hidden class=anchor aria-hidden=true href=#secrets>#</a></h2><ul><li>working with a Secret is like working with a ConfigMap</li></ul><h3 id=secret-use-with-string-literals>Secret: Use with string literals<a hidden class=anchor aria-hidden=true href=#secret-use-with-string-literals>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># First, create a secret using a string literal.</span>
</span></span><span class=line><span class=cl>kubectl create secret generic api-creds <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --from-literal<span class=o>=</span><span class=nv>key</span><span class=o>=</span>mysupersecretapikey
</span></span><span class=line><span class=cl><span class=c1># the get command verifies that the secret was created</span>
</span></span><span class=line><span class=cl>kubectl get secret
</span></span><span class=line><span class=cl><span class=c1># use the DESCRIBE command to verify our secret is indeed a secret</span>
</span></span><span class=line><span class=cl><span class=c1># and check that you don’t see any secret, written using displayed text</span>
</span></span><span class=line><span class=cl>kubectl describe secret api-creds
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>kubectl get secret api-creds -o YAML
</span></span></code></pre></div><h3 id=secret-use-with-environment-variables>Secret: use with environment variables<a hidden class=anchor aria-hidden=true href=#secret-use-with-environment-variables>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>env</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>API_CREDS</span><span class=w> </span><span class=c># process.env.API_CREDS</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>valueFrom</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>secretKeyRef</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>api-creds</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>key</span><span class=p>:</span><span class=w> </span><span class=l>key</span><span class=w>
</span></span></span></code></pre></div><h3 id=secret-use-with-volume-mounts>Secret: use with volume mounts<a hidden class=anchor aria-hidden=true href=#secret-use-with-volume-mounts>#</a></h3><ul><li>Each container in the descriptor file has its own volumeMount but shares the volume</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl create secret generic api-creds <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --from-literal<span class=o>=</span><span class=nv>key</span><span class=o>=</span>mysupersecretapikey
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>containers</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>hello-kubernetes</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>cham11ng/myapp:latest</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span>- <span class=nt>containerPort</span><span class=p>:</span><span class=w> </span><span class=m>8080</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=c># use a volume for the secret with a corresponding volumeMount</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>volumeMounts</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>api-creds</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=c># The api-creds secret is mounted as a file at /etc/api/api-creds</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>mountPath</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;/etc/api&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>readOnly</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>volumes</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>api-creds</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>secrets</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>secretName</span><span class=p>:</span><span class=w> </span><span class=l>api-creds</span><span class=w>
</span></span></span></code></pre></div><h2 id=service-binding>Service Binding<a hidden class=anchor aria-hidden=true href=#service-binding>#</a></h2><ul><li>is the process needed to consume external Services or backing Services, including REST APIs, databases, and event buses in our applications.</li><li>manages configuration and credentials for back-end Services while protecting sensitive data.</li><li>In addition, Service binding makes Service credentials available to you automatically as a Secret.</li><li>consumes the external Service by binding the application to a deployment.</li><li>Then, the application code uses the credentials from the binding and calls the corresponding Service.</li><li>Here you can see an architectural diagram that illustrates the binding of a Kubernetes Cluster to an external Service.</li><li>Next, let&rsquo;s learn the steps required to bind the Service to your application.</li></ul><h3 id=architecture>Architecture<a hidden class=anchor aria-hidden=true href=#architecture>#</a></h3><p><input type=checkbox id=zoomCheck-c32f8 hidden>
<label for=zoomCheck-c32f8><img class=zoomCheck loading=lazy decoding=async src=img/service-binding-architecture.webp alt="Service Binding Architecture"></label></p><h3 id=ibm-service-binding>IBM Service binding<a hidden class=anchor aria-hidden=true href=#ibm-service-binding>#</a></h3><ul><li>Service binding quickly creates Service credentials for an IBM Cloud Service.</li><li>You create the Service credentials using IBM’s public cloud Service endpoint and then store or “bind” your Service credentials in a Kubernetes Secret in your Cluster.</li><li>Here’s how to bind an IBM Cloud Service to your Cluster:<ol><li>Provision an instance of the Service</li><li>Bind the Service to your Cluster to create Service credentials for your Service that use the public cloud Service endpoint</li><li>Store and retrieve the Service credentials in a Kubernetes Secret</li><li>Configure your app to access the Service credentials in the Kubernetes Secret</li></ol></li></ul><h2 id=commands>Commands<a hidden class=anchor aria-hidden=true href=#commands>#</a></h2><table><thead><tr><th>Command</th><th>Description</th></tr></thead><tbody><tr><td>kubectl autoscale deployment</td><td>Autoscales a Kubernetes Deployment.</td></tr><tr><td>kubectl create configmap</td><td>Creates a ConfigMap resource.</td></tr><tr><td>kubectl get deployments -o wide</td><td>Lists deployments with details.</td></tr><tr><td>kubectl get hpa</td><td>Lists Horizontal Pod Autoscalers (hpa)</td></tr><tr><td>kubectl scale deployment</td><td>Scales a deployment.</td></tr><tr><td>kubectl set image deployment</td><td>Updates the current deployment.</td></tr><tr><td>kubectl rollout</td><td>Manages the rollout of a resource.</td></tr><tr><td>kubectl rollout restart</td><td>Restarts the resource so that the containers restart.</td></tr><tr><td>kubectl rollout undo</td><td>Rollbacks the resource.</td></tr></tbody></table></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy;
2025
<a href=http://localhost:1313/>Agus Fahmi Aji Pramana's Blogging Site</a></span>
| <span><a href=/terms-and-conditions>Terms and Conditions</a>
</span>|
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a>
&
      <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span><br></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})}),document.addEventListener("DOMContentLoaded",function(){var e=document.querySelectorAll(".project");e.forEach(function(e){e.addEventListener("click",function(t){t.target.closest(".small-icons")||window.open(e.getAttribute("data-project-link"),"_blank").focus()})})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>