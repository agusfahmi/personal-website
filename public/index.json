[{"content":"Objectives In this lab, I will:\nPull an image from Docker Hub Run an image as a container using docker Build an image using a Dockerfile Push an image to IBM Cloud Container Registry Verify Environment and Command line tools $ docker --version Docker version 24.0.7, build afdd53b $ ibmcloud version ibmcloud version 2.20.0+f382323-2023-09-19T20:06:39+00:00 $ [ ! -d \u0026#39;CC201\u0026#39; ] \u0026amp;\u0026amp; git clone \\ https://github.com/ibm-developer-skills-network/CC201.git $ cd CC201/labs/1_ContainersAndDocker/ $ ls Dockerfile app.js package.json Pulling an image from Docker Hub and running as a container # listing images, initially empty $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE # pull docker hello-world from Docker Hub $ docker pull hello-world Using default tag: latest latest: Pulling from library/hello-world c1ec31eb5944: Pull complete Digest: sha256:encoded-code Status: Downloaded newer image for hello-world:latest docker.io/library/hello-world:latest # listing images again $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE hello-world latest d2c94e258dcb 9 months ago 13.3kB # running `hello-world` image as a container $ docker run hello-world Hello from Docker! # list the containers to see it exited successfully $ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 9c755c404f2c hello-world \u0026#34;/hello\u0026#34; 1 sec Exited (0) 3 sec reverent_hoover # remove container $ docker container rm 9c755c404f2c 9c755c404f2c # verify the container has been removed $ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES Build an image using Dockerfile $ docker build . -t myimage:v1 =\u0026gt; =\u0026gt; naming to docker.io/library/myimage:v1 0.0s Run the image as a container # run container docker run -dp 8080:8080 myimage:v1 349f19114bfcb01a40328092a687f0cd18da3c97e07c8bb6e99e87aaeb4c83d4 # ping the application curl localhost:8080 Hello world from 349f19114bfc! Your app is up and running! # stop the list of all running container docker stop $(docker ps -q) 349f19114bfc # check container has stopped or not docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES Push the image to IBM Cloud Container Registry # environment has already logged into IBM Cloud account $ ibmcloud target API endpoint: https://cloud.ibm.com Region: us-south User: ServiceId-9917a3b5-23f8-4163-a5f6-463699167d84 (sn-labs-x) Account: QuickLabs - IBM Skills Network (xyz) Resource group: No resource group targeted CF API endpoint: Org: Space: # environment has created ICR namespace for you $ ibmcloud cr namespaces Listing namespaces for account \u0026#39;IBM Skills Network\u0026#39; in registry \u0026#39;us.icr.io\u0026#39;. Namespace sn-labs-x sn-labsassets OK # setting appropriate region where the namespace reside $ ibmcloud cr region-set us-south The region is set to \u0026#39;us-south\u0026#39;, the registry is \u0026#39;us.icr.io\u0026#39; OK # login to local Docker daemon into IBM Cloud Registry $ ibmcloud cr login Logging \u0026#39;docker\u0026#39; in to \u0026#39;us.icr.io\u0026#39;... Logged in to \u0026#39;us.icr.io\u0026#39;. OK # export namespace as environment variable $ export MY_NAMESPACE=sn-labs-$USERNAME # tag image so that it can be pushed $ docker tag myimage:v1 us.icr.io/$MY_NAMESPACE/hello-world:1 # push the newly tagged image to ICR $ docker push us.icr.io/$MY_NAMESPACE/hello-world:1 # to only view images within a specific namespace $ ibmcloud cr images --restrict $MY_NAMESPACE ibmcloud cr images --restrict $MY_NAMESPACE Listing images... Repository Tag Digest Namespace Created us.icr.io/sn-labs-x/hello-world 1 b63783d1b808 sn-labs-x z sec OK Reference https://www.coursera.org/learn/ibm-containers-docker-kubernetes-openshift ","permalink":"http://localhost:1313/blogs/training/introduction-to-containers/introduction-to-containers-docker-ibm-cloud-container-registry-lab/","summary":"Explore and work on a hand-on Container, Docker and IBM Container Registry","title":"Introduction to Containers, Docker, and IBM Cloud Container Registry Lab"},{"content":"Mission Introduction In this mission, I have to setup free tier of AWS and Google Cloud Account. Multi-Cloud is when any organization uses different cloud service providers to successfully run an application. It is also referred as a strategy of using the best services from two or more Cloud providers.\nInside Manager IAM user to provide programmatic access to Terraform user. We use Terraform (IaC) to provision Google Cloud SQL, GKE Cluster and AWS S3 Bucket using Google CloudShell. Join Community AWS and Google Cloud Account Setup AWS Google Cloud Free Trail Begin Hands-on Project Setting up Terraform user in AWS IAM Running commands in Google CloudShell Initializing Terraform AWS S3 Bucket Setup via. Terraform Google SQL and Kubernetes Cluster Setup via. Terraform SQL Kubernetes Cluster SQL Network Configuration Mission Accomplished SQL Instance Kubernetes LinkedIn Post Invite to The Cloud Bootcamp https://e.thecloudbootcamp.com/d8z5zO ","permalink":"http://localhost:1313/blogs/training/intensive-cloud-training/mission-1-challenge/","summary":"Learning and Outcome from Mission 1 Challenge","title":"Mission 1 Challenge - Intensive Cloud Training"},{"content":"Lab Overview This lab is part of a course named \u0026ldquo;Introduction to Containers w/ Docker, Kubernetes \u0026amp; OpenShift\u0026rdquo;. In this lab, I will create an IBM Cloud Container Registry. After completing this lab, we will be able to:\nDescribe the IBM Cloud Container Registry service Create a Container Registry namespace To perform this lab, we need IBM Cloud which can be activated without the credit card detail instead with the feature code that\u0026rsquo;s included in this course.\nActivate IBM Cloud using Feature Code Navigating IBM Cloud Dashboard Navigating to Container Registry Created Namespaces Browsing Namespace Reference https://www.coursera.org/learn/ibm-containers-docker-kubernetes-openshift ","permalink":"http://localhost:1313/blogs/training/introduction-to-containers/setup-ibm-cloud-container-registry-namespace-lab/","summary":"Explore and work on a hand-on IBM Cloud Container Registry Namespace","title":"Setup IBM Cloud Container Registry Namespace Lab"},{"content":"Mission Introduction In this mission, I have my Google Cloud SQL instance and Google Kubernetes Engine Cluster up and running from the previous Mission 1. Going further, I\u0026rsquo;ll have to setup MySQL database, deploy image of application to Google Container Registry, deploy the app in cluster as GKE Workload. The main goal is deploy an app and make it publicly accessible.\nInside Manager IAM user to provide programmatic access to AWS S3 Bucket from Google Kubernetes Cluster app. Manually setup user in SQL Cloud, create database and table which will be used by the application. Build and upload the docker image in GCR (Google Container Registry). Connect the previous made application cluster to Google Cloud Container cluster. Apply the application Kubernetes configuration to run upload GCR image in a cluster Once, the app is up and running we use the public IP address to access the application. Begin Hands-on Project User creation for Kubernetes cluster app to connect S3 Create user to access SQL Instance Create table in mysql database Build Docker image and push it to GCR Container Registry of App Connect to GKE cluster Mission Accomplished Deployed application in Cluster as Workload Browsing the application Invite to The Cloud Bootcamp https://e.thecloudbootcamp.com/d8z5zO ","permalink":"http://localhost:1313/blogs/training/intensive-cloud-training/mission-2-challenge/","summary":"Learning and Outcome from Mission 2 Challenge","title":"Mission 2 Challenge - Intensive Cloud Training"},{"content":"Objectives In this lab, I will:\nUse the kubectl CLI Create a Kubernetes Pod Create a Kubernetes Deployment Create a ReplicaSet that maintains a set number of replicas Witness Kubernetes load balancing in action Cloud IDE Verify the environment and command line tools $ kubectl version WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short. Use --output=yaml|json to get the full version. Client Version: version.Info { Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;27\u0026#34;, GitVersion:\u0026#34;v1.27.6\u0026#34;, GitCommit:\u0026#34;741c8db18a52787d734cbe4795f0b4ad860906d6\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2023-09-13T09:21:34Z\u0026#34;, GoVersion:\u0026#34;go1.20.8\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34; } Kustomize Version: v5.0.1 Server Version: version.Info { Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;26\u0026#34;, GitVersion:\u0026#34;v1.26.13+IKS\u0026#34;, GitCommit:\u0026#34;fbfb72fd9f7d07c29a23849656a07ce69833b650\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2024-01-18T18:12:58Z\u0026#34;, GoVersion:\u0026#34;go1.20.13\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34; } $ [ ! -d \u0026#39;CC201\u0026#39; ] \u0026amp;\u0026amp; git clone \\ https://github.com/ibm-developer-skills-network/CC201.git $ ls Dockerfile app.js hello-world-apply.yaml hello-world-create.yaml package.json Using the kubectl CLI Kubernetes namespaces enable to virtualize a cluster. We already have access to one namespace in a Kubernetes cluster, kubectl is already set to target that cluster and namespace.\n# kubectl requires configuration so that it targets the appropriate cluster. $ kubectl config get-clusters NAME labs-prod-kubernetes-sandbox/c8ana0 # kubectl context is a group of access parameters, including a cluster, # a user, and a namespace. $ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE * x-context labs-prod-kubernetes-sandbox/c8ana0 x sn-labs-x # list all the pods in your namespace. $ kubectl get pods No resources found in sn-labs-x namespace. Create a Pod with an imperative command We create a pod that run the hello-world image you built and push it to IBM Cloud Container Registry in the previous lab.\n# exporting namespace as an environment variable so that # it can be used in subsequent commands. $ export MY_NAMESPACE=sn-labs-$USERNAME # build and push the image $ docker build -t us.icr.io/$MY_NAMESPACE/hello-world:1 . \u0026amp;\u0026amp; docker push us.icr.io/$MY_NAMESPACE/hello-world:1 # run the hello-world image as a container in Kubernetes $ kubectl run hello-world --image us.icr.io/$MY_NAMESPACE/hello-world:1 --overrides=\u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;template\u0026#34;:{\u0026#34;spec\u0026#34;:{\u0026#34;imagePullSecrets\u0026#34;:[{\u0026#34;name\u0026#34;:\u0026#34;icr\u0026#34;}]}}}}\u0026#39; pod/hello-world created # list the pods in your namespace $ kubectl get pods NAME READY STATUS RESTARTS AGE hello-world 1/1 Running 0 11s # the wide option for the output to get more details about the resource $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES hello-world 1/1 Running 0 2m31s 172.17.153.207 10.241.64.6 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; # describe the pod to get more details about it $ kubectl describe pod hello-world # delete the pod $ kubectl delete pod hello-world # list the pods to verify that none exist $ kubectl get pods Create a Pod with imperative object configuration # hello-world-create.yaml apiVersion: v1 kind: Pod metadata: name: hello-world spec: containers: - name: hello-world image: us.icr.io/sn-labs-x/hello-world:1 ports: - containerPort: 8080 imagePullSecrets: - name: icr # imperative create a Pod using the provided configuration file $ kubectl create -f hello-world-create.yaml pod/hello-world created # list the pods in your namespace $ kubectl get pods NAME READY STATUS RESTARTS AGE hello-world 1/1 Running 0 15s # delete the pod $ kubectl delete pod hello-world pod \u0026#34;hello-world\u0026#34; deleted # list the pods to verify that non exist $ kubectl get pods No resources found in sn-labs-x namespace. Create a Pod with declarative command # hello-world-apply.yaml apiVersion: apps/v1 kind: Deployment metadata: generation: 1 labels: run: hello-world name: hello-world spec: replicas: 3 selector: matchLabels: run: hello-world strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 1 type: RollingUpdate template: metadata: labels: run: hello-world spec: containers: - image: us.icr.io/\u0026lt;my_namespace\u0026gt;/hello-world:1 imagePullPolicy: Always name: hello-world ports: - containerPort: 8080 protocol: TCP resources: limits: cpu: 2m memory: 30Mi requests: cpu: 1m memory: 10Mi imagePullSecrets: - name: icr dnsPolicy: ClusterFirst restartPolicy: Always securityContext: {} terminationGracePeriodSeconds: 30 # to set this configuration as the desired state in Kubernetes $ kubectl apply -f hello-world-apply.yaml deployment.apps/hello-world created # get the deployments to ensure that the deployment was created $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE hello-world 3/3 3 3 56s # list the pods to ensure that three replicas exist $ kubectl get pods NAME READY STATUS RESTARTS AGE hello-world-5d7cc94cb5-npdtg 1/1 Running 0 92s hello-world-5d7cc94cb5-pskk6 1/1 Running 0 92s hello-world-5d7cc94cb5-rks8g 1/1 Running 0 92s # delete the pod and created a new one immediately $ kubectl delete pod hello-world-5d7cc94cb5-npdtg \u0026amp;\u0026amp; kubectl get pods pod \u0026#34;hello-world-5d7cc94cb5-npdtg\u0026#34; deleted NAME READY STATUS RESTARTS AGE hello-world-5d7cc94cb5-pskk6 1/1 Running 0 4m29s hello-world-5d7cc94cb5-rks8g 1/1 Running 0 4m29s hello-world-5d7cc94cb5-xn2tp 1/1 Running 0 34s Load balancing the application // app.js var express = require(\u0026#39;express\u0026#39;); var os = require(\u0026#39;os\u0026#39;); var hostname = os.hostname(); var app = express(); app.get(\u0026#39;/\u0026#39;, function (req, res) { res.send(\u0026#39;Hello world, \u0026#39; + hostname + \u0026#39;! Your app is up and running!\\n\u0026#39;); }); app.listen(8080, function () { console.log(\u0026#39;Sample app is listening on port 8080.\u0026#39;); }); # Dockerfile FROM node:9.4.0-alpine COPY app.js . COPY package.json . RUN npm install \u0026amp;\u0026amp;\\ apk update \u0026amp;\u0026amp;\\ apk upgrade EXPOSE 8080 CMD node app.js # access the application, expose it to the internet using Kubernetes service $ kubectl expose deployment/hello-world # list service in order to see that this service was created. $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE hello-world ClusterIP 172.21.142.190 \u0026lt;none\u0026gt; 8080/TCP 2m41s # create a proxy to make it accessible outside of the cluster $ kubectl proxy Starting to serve on 127.0.0.1:8001 # ping the application to get a response $ curl -L localhost:8001/api/v1/namespaces/sn-labs-$USERNAME/services/hello-world/proxy Hello world, hello-world-5d7cc94cb5-xn2tp! Your app is up and running! # run the command in loop, observe the Kubernetes load balancing the request # across the three replicas, each request could hit a different instance # of our application. $ for i in `seq 10`; do curl -L localhost:8001/api/v1/namespaces/sn-labs-$USERNAME/services/hello-world/proxy; done Hello world, hello-world-5d7cc94cb5-pskk6! Your app is up and running! Hello world, hello-world-5d7cc94cb5-pskk6! Your app is up and running! Hello world, hello-world-5d7cc94cb5-xn2tp! Your app is up and running! Hello world, hello-world-5d7cc94cb5-pskk6! Your app is up and running! Hello world, hello-world-5d7cc94cb5-xn2tp! Your app is up and running! Hello world, hello-world-5d7cc94cb5-pskk6! Your app is up and running! Hello world, hello-world-5d7cc94cb5-rks8g! Your app is up and running! Hello world, hello-world-5d7cc94cb5-rks8g! Your app is up and running! Hello world, hello-world-5d7cc94cb5-pskk6! Your app is up and running! Hello world, hello-world-5d7cc94cb5-xn2tp! Your app is up and running! # delete the deployment and service $ kubectl delete deployment/hello-world service/hello-world deployment.apps \u0026#34;hello-world\u0026#34; deleted service \u0026#34;hello-world\u0026#34; deleted Reference https://www.coursera.org/learn/ibm-containers-docker-kubernetes-openshift https://github.com/ibm-developer-skills-network/CC201 ","permalink":"http://localhost:1313/blogs/training/introduction-to-containers/introduction-to-kubernetes-lab/","summary":"Hands-on kubectl CLI, Kubernetes Pod, Deployment, ReplicaSet and load balancing","title":"Introduction to Kubernetes Lab"},{"content":"Mission Introduction In this mission, we\u0026rsquo;ll perform the database migration and files migration in Google Cloud SQL and AWS S3 respectively. After the migration process is done, we successfully finished all the mission from the trainings. Then we start to deprovision and delete all the resources to avoid charges in Cloud Services.\nInside We migrate the SQL dump file data to Google Cloud SQL using Google Cloud Shell. Then, we migrate the PDF files to S3 Bucket using AWS CloudShell. Deprovision and delete all resource using Terraform. Begin Hands-on Project Database migration to Google Cloud SQL using MySQL dump PDF Files Migration to S3 Bucket using AWS CloudShell Uploaded files in AWS S3 Bucket Mission Accomplished Navigating records in Application Accessing the file from Application Cleanup Disabling deletion protection in Terraform file Disabling deletion protection in tfstate Deleted resources Certificate of Completion ","permalink":"http://localhost:1313/blogs/training/intensive-cloud-training/mission-3-challenge/","summary":"Learning and Outcome from Mission 3 Challenge","title":"Mission 3 Challenge - Intensive Cloud Training"},{"content":"Objectives In this lab, I will:\nScale an application with a ReplicaSet Apply rolling updates to an application Use a ConfigMap to store application configuration Autoscale the application using Horizontal Pod Autoscaler Verify the environment and command line tools # browse to the given path $ cd CC201/labs/3_K8sScaleAndUpdate/ Build and push application image to IBM Cloud Container Registry # export namespace to an environment variable to be used later export MY_NAMESPACE=sn-labs-$USERNAME # build and push the image to the IBM Cloud Container Registry docker build -t us.icr.io/$MY_NAMESPACE/hello-world:1 . docker push us.icr.io/$MY_NAMESPACE/hello-world:1 Deploy the application to Kubernetes apiVersion: apps/v1 kind: Deployment metadata: name: hello-world spec: selector: matchLabels: run: hello-world template: metadata: labels: run: hello-world spec: containers: - name: hello-world image: us.icr.io/sn-labs-x/hello-world:1 ports: - containerPort: 8080 resources: limits: cpu: 2m memory: 30Mi requests: cpu: 1m memory: 10Mi imagePullSecrets: - name: icr # Running image as a deployment $ kubectl apply -f deployment.yaml deployment.apps/hello-world created # Listing pods until the status is \u0026#34;Running\u0026#34; $ kubectl get pods NAME READY STATUS RESTARTS AGE hello-world-55f8cdd55-hn7v8 1/1 Running 0 17s # to expose it to internet via a Kubernetes Service # create a service of type ClusterIP $ kubectl expose deployment/hello-world service/hello-world exposed # to make this externally accessible $ kubectl proxy Starting to serve on 127.0.0.1:8001 # ping the application to get a response $ curl -L localhost:8001/api/v1/namespaces/sn-labs-$USERNAME/services/hello-world/proxy Hello world from hello-world-55f8cdd55-hn7v8! Your app is up and running! Scaling the application using a ReplicaSet # to scale up Deployment $ kubectl scale deployment hello-world --replicas=3 deployment.apps/hello-world scaled # to make sure the three pods created instead of just one $ kubectl get pods NAME READY STATUS RESTARTS AGE hello-world-55f8cdd55-hn7v8 1/1 Running 0 8m51s hello-world-55f8cdd55-l2dqr 1/1 Running 0 9s hello-world-55f8cdd55-n9rkd 1/1 Running 0 9s # ping application multiple times to ensure load-balancing across replicas $ for i in `seq 10`; do curl -L localhost:8001/api/v1/namespaces/sn-labs-$USERNAME/services/hello-world/proxy; done Hello world from hello-world-55f8cdd55-l2dqr! Your app is up and running! Hello world from hello-world-55f8cdd55-hn7v8! Your app is up and running! Hello world from hello-world-55f8cdd55-n9rkd! Your app is up and running! Hello world from hello-world-55f8cdd55-l2dqr! Your app is up and running! Hello world from hello-world-55f8cdd55-l2dqr! Your app is up and running! Hello world from hello-world-55f8cdd55-n9rkd! Your app is up and running! Hello world from hello-world-55f8cdd55-l2dqr! Your app is up and running! Hello world from hello-world-55f8cdd55-n9rkd! Your app is up and running! Hello world from hello-world-55f8cdd55-l2dqr! Your app is up and running! Hello world from hello-world-55f8cdd55-l2dqr! Your app is up and running! # to scale down Deployment $ kubectl scale deployment hello-world --replicas=1 deployment.apps/hello-world scaled # check the pods to see that two are deleted or being deleted $ kubectl get pods NAME READY STATUS RESTARTS AGE hello-world-55f8cdd55-hn7v8 1/1 Running 0 13m hello-world-55f8cdd55-pn4vr 1/1 Terminating 0 14s hello-world-55f8cdd55-rgvfp 1/1 Terminating 0 14s # to ensure only one pod exists $ kubectl get pods NAME READY STATUS RESTARTS AGE hello-world-55f8cdd55-hn7v8 1/1 Running 0 13m Perform rolling updates // Change \u0026#34;Hello world from\u0026#34; to \u0026#34;Welcome to\u0026#34; var express = require(\u0026#39;express\u0026#39;); var os = require(\u0026#39;os\u0026#39;); var hostname = os.hostname(); var app = express(); app.get(\u0026#39;/\u0026#39;, function (req, res) { res.send(\u0026#39;Welcome from \u0026#39; + hostname + \u0026#39;! Your app is up and running!\\n\u0026#39;); }); app.listen(8080, function () { console.log(\u0026#39;Sample app is listening on port 8080.\u0026#39;); }); # Rebuild the version 2 and push to ICR. $ docker build -t us.icr.io/$MY_NAMESPACE/hello-world:2 . \u0026amp;\u0026amp; docker push us.icr.io/$MY_NAMESPACE/hello-world:2 # List images in Container Registry that is pushed. $ ibmcloud cr images Listing images... Repository Tag Digest Namespace Created Size Security status us.icr.io/sn-labs-x/hello-world 1 5cf5e32e9f03 sn-labs-x 30 minutes ago 28 MB - us.icr.io/sn-labs-x/hello-world 2 2e591de8e071 sn-labs-x 1 minute ago 28 MB - # update the deployment to use the version instead $ kubectl set image deployment/hello-world hello-world=us.icr.io/$MY_NAMESPACE/hello-world:2 deployment.apps/hello-world image updated # get a status of the rolling update by using the following command $ kubectl rollout status deployment/hello-world # get the deployment with the `wide` option to see the new tag used $ kubectl get deployments -o wide NAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR hello-world 1/1 1 1 20m hello-world us.icr.io/sn-labs-x/hello-world:2 run=hello-world # ping the application to ensure that the new welcome message is displayed $ curl -L localhost:8001/api/v1/namespaces/sn-labs-$USERNAME/services/hello-world/proxy Welcome to hello-world-55df49f45b-zjjf8! Your app is up and running! # can rollback the deployment $ kubectl rollout undo deployment/hello-world deployment.apps/hello-world rolled back # get a status of the rolling update $ kubectl rollout status deployment/hello-world deployment \u0026#34;hello-world\u0026#34; successfully rolled out # get the deployment with the `wide` option to see that the old tag is used $ kubectl get deployments -o wide NAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR hello-world 1/1 1 1 31m hello-world us.icr.io/sn-labs-x/hello-world:1 run=hello-world # ping application to ensure that the earlier \u0026#34;Hello World...app is running\u0026#34; curl -L localhost:8001/api/v1/namespaces/sn-labs-$USERNAME/services/hello-world/proxy Hello world from hello-world-55f8cdd55-wslb7! Your app is up and running! Using a ConfigMap to store configuration apiVersion: apps/v1 kind: Deployment metadata: name: hello-world spec: selector: matchLabels: run: hello-world template: metadata: labels: run: hello-world spec: containers: - name: hello-world image: us.icr.io/sn-labs-x/hello-world:3 ports: - containerPort: 8080 envFrom: - configMapRef: name: app-config imagePullSecrets: - name: icr // app.js var express = require(\u0026#39;express\u0026#39;); var app = express(); app.get(\u0026#39;/\u0026#39;, function (req, res) { res.send(process.env.MESSAGE + \u0026#39;\\n\u0026#39;); }); app.listen(8080, function () { console.log(\u0026#39;Sample app is listening on port 8080.\u0026#39;); }); # create a ConfigMap that contains a new message. $ kubectl create configmap app-config --from-literal=MESSAGE=\u0026#34;This message came from a ConfigMap!\u0026#34; configmap/app-config created # build version 3 and push a new image $ docker build -t us.icr.io/$MY_NAMESPACE/hello-world:3 . \u0026amp;\u0026amp; docker push us.icr.io/$MY_NAMESPACE/hello-world:3 # apply the new deployment $ kubectl apply -f deployment-configmap-env-var.yaml deployment.apps/hello-world configured # ping application, message should come from ConfigMap! $ curl -L localhost:8001/api/v1/namespaces/sn-labs-$USERNAME/services/hello-world/proxy This message came from a ConfigMap! # change message without rebuilding the image $ kubectl delete configmap app-config configmap \u0026#34;app-config\u0026#34; deleted $ kubectl create configmap app-config --from-literal=MESSAGE=\u0026#34;This message is different, and you didn\u0026#39;t have to rebuild the image!\u0026#34; configmap/app-config created # restart the deployment so the container restart $ kubectl rollout restart deployment hello-world deployment.apps/hello-world restarted # ping application if the new message is from environment variable $ curl -L localhost:8001/api/v1/namespaces/sn-labs-$USERNAME/services/hello-world/proxy This message is different, and you didn\u0026#39;t have to rebuild the image! Autoscale the hello-world application using Horizontal Pod Autoscaler apiVersion: apps/v1 kind: Deployment metadata: name: hello-world spec: selector: matchLabels: run: hello-world template: metadata: labels: run: hello-world spec: containers: - name: hello-world image: us.icr.io/sn-labs-x/hello-world:1 ports: - containerPort: 8080 name: http resources: limits: cpu: 50m requests: cpu: 20m imagePullSecrets: - name: icr # apply the deployment $ kubectl apply -f deployment.yaml deployment.apps/hello-world configured # autoscale `hello-world` deployment $ kubectl autoscale deployment hello-world --cpu-percent=5 --min=1 --max=10 horizontalpodautoscaler.autoscaling/hello-world autoscaled # initial observation the replicas increase in accordance with the autoscaling $ kubectl get hpa hello-world --watch NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE hello-world Deployment/hello-world 0%/5% 1 10 1 114s # spam the app with multiple requests for increasing the load # in separate command $ for i in `seq 100000`; do curl -L localhost:8001/api/v1/namespaces/sn-labs-$USERNAME/services/hello-world/proxy; done Hello world from hello-world-6c59687774-l26sv! Your app is up and running! Hello world from hello-world-6c59687774-7t8kd! Your app is up and running! dial tcp 172.17.153.203:8080: connect: connection refusedHello world from hello-world-6c59687774-sd5px! Your app is up and running! Hello world from hello-world-6c59687774-4h4zr! Your app is up and running! Hello world from hello-world-6c59687774-hxhq8! Your app is up and running! # observe again the replicas increase in accordance with the autoscaling $ kubectl get hpa hello-world --watch NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE hello-world Deployment/hello-world 0%/5% 1 10 1 9m41s hello-world Deployment/hello-world 0%/5% 1 10 1 10m hello-world Deployment/hello-world 105%/5% 1 10 1 10m hello-world Deployment/hello-world 105%/5% 1 10 4 11m hello-world Deployment/hello-world 105%/5% 1 10 8 11m hello-world Deployment/hello-world 30%/5% 1 10 10 11m # observe the details of horizontal pod autoscaler $ kubectl get hpa hello-world NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE hello-world Deployment/hello-world 4%/5% 1 10 10 14m # delete the deployment $ kubectl delete deployment hello-world deployment.apps \u0026#34;hello-world\u0026#34; deleted # delete the service $ kubectl delete service hello-world service \u0026#34;hello-world\u0026#34; deleted Reference https://www.coursera.org/learn/ibm-containers-docker-kubernetes-openshift https://github.com/ibm-developer-skills-network/CC201 ","permalink":"http://localhost:1313/blogs/training/introduction-to-containers/scaling-and-updating-applications/","summary":"Hands-on lab on scaling, applying rolling updates, ConfigMap and Horizontal Pod Autoscaler","title":"Scaling and Updating Applications"},{"content":"Objectives OpenShift projects are Kubernetes namespaces with additional administrative functions. Therefore, projects also provide isolation within an OpenShift cluster. You already have access to one project in an OpenShift cluster, and oc is already set to target that cluster and project. In this lab, I will:\nUse the oc CLI (OpenShift command line interface) Use the OpenShift web console Build and deploy an application using s2i (‘Source-to-image’ build strategy) Inspect a BuildConfig and an ImageStream Autoscale the application Verify the environment and command line tools $ oc version Client Version: 4.13.7 Kustomize Version: v4.5.7 Kubernetes Version: v1.26.11+4ad3e1b Use the oc CLI # list the pods in the namespace $ oc get pods NAME READY STATUS RESTARTS AGE openshift-web-console-576f574d69-dgrr5 2/2 Running 0 28m openshift-web-console-576f574d69-fpldg 2/2 Running 0 28m # additionally, get OpenShift specific options # haven’t created a BuildConfig yet, this will not return any resources. $ oc get buildconfigs No resources found in sn-labs-x namespace. # view the OpenShift project that is currently in use $ oc project Using project \u0026#34;sn-labs-x\u0026#34; from context named \u0026#34;x-context\u0026#34; on server \u0026#34;https://c109-e.us-east.containers.cloud.ibm.com:30807\u0026#34;. Note: This project is specific to me and provides isolation within the cluster so that I can deploy my own application.\nUse the OpenShift web console Deploy an application in the web console Creating Git Repo Browsing Application Topology Browsing OpenShift Console View application in the web console kind: BuildConfig apiVersion: build.openshift.io/v1 metadata: annotations: app.openshift.io/vcs-ref: \u0026#39;\u0026#39; app.openshift.io/vcs-uri: \u0026#39;https://github.com/sclorg/nodejs-ex.git\u0026#39; openshift.io/generated-by: OpenShiftWebConsole resourceVersion: \u0026#39;1283754444\u0026#39; name: nodejs-ex-git uid: 6eaa6e21-14ad-496c-a151-36ac8f1f7dbf creationTimestamp: \u0026#39;2024-02-13T04:26:19Z\u0026#39; generation: 2 managedFields: - manager: Mozilla operation: Update apiVersion: build.openshift.io/v1 time: \u0026#39;2024-02-13T04:26:19Z\u0026#39; fieldsType: FieldsV1 fieldsV1: \u0026#39;f:metadata\u0026#39;: \u0026#39;f:annotations\u0026#39;: .: {} \u0026#39;f:app.openshift.io/vcs-ref\u0026#39;: {} \u0026#39;f:app.openshift.io/vcs-uri\u0026#39;: {} \u0026#39;f:openshift.io/generated-by\u0026#39;: {} \u0026#39;f:labels\u0026#39;: .: {} \u0026#39;f:app\u0026#39;: {} \u0026#39;f:app.kubernetes.io/component\u0026#39;: {} \u0026#39;f:app.kubernetes.io/instance\u0026#39;: {} \u0026#39;f:app.kubernetes.io/name\u0026#39;: {} \u0026#39;f:app.kubernetes.io/part-of\u0026#39;: {} \u0026#39;f:app.openshift.io/runtime\u0026#39;: {} \u0026#39;f:app.openshift.io/runtime-version\u0026#39;: {} \u0026#39;f:spec\u0026#39;: \u0026#39;f:output\u0026#39;: \u0026#39;f:to\u0026#39;: {} \u0026#39;f:runPolicy\u0026#39;: {} \u0026#39;f:source\u0026#39;: \u0026#39;f:contextDir\u0026#39;: {} \u0026#39;f:git\u0026#39;: .: {} \u0026#39;f:uri\u0026#39;: {} \u0026#39;f:type\u0026#39;: {} \u0026#39;f:strategy\u0026#39;: \u0026#39;f:sourceStrategy\u0026#39;: .: {} \u0026#39;f:from\u0026#39;: {} \u0026#39;f:type\u0026#39;: {} \u0026#39;f:triggers\u0026#39;: {} - manager: openshift-apiserver operation: Update apiVersion: build.openshift.io/v1 time: \u0026#39;2024-02-13T04:26:19Z\u0026#39; fieldsType: FieldsV1 fieldsV1: \u0026#39;f:status\u0026#39;: \u0026#39;f:imageChangeTriggers\u0026#39;: {} \u0026#39;f:lastVersion\u0026#39;: {} namespace: sn-labs-x labels: app: nodejs-ex-git app.kubernetes.io/component: nodejs-ex-git app.kubernetes.io/instance: nodejs-ex-git app.kubernetes.io/name: nodejs-ex-git app.kubernetes.io/part-of: nodejs-ex-git-app app.openshift.io/runtime: nodejs app.openshift.io/runtime-version: 16-ubi8 spec: nodeSelector: null output: to: kind: ImageStreamTag name: \u0026#39;nodejs-ex-git:latest\u0026#39; resources: {} successfulBuildsHistoryLimit: 5 failedBuildsHistoryLimit: 5 strategy: type: Source sourceStrategy: from: kind: ImageStreamTag namespace: openshift name: \u0026#39;nodejs:16-ubi8\u0026#39; postCommit: {} source: type: Git git: uri: \u0026#39;https://github.com/sclorg/nodejs-ex.git\u0026#39; contextDir: / triggers: - type: Generic generic: secretReference: name: nodejs-ex-git-generic-webhook-secret - type: GitHub github: secretReference: name: nodejs-ex-git-github-webhook-secret - type: ImageChange imageChange: {} - type: ConfigChange runPolicy: Serial status: lastVersion: 1 imageChangeTriggers: - lastTriggeredImageID: \u0026gt;- image-registry.openshift-image-registry.svc:5000/openshift/nodejs@sha256:a192d3d1ff9933e9f0ebf1401c3c7f016705981c7bed1e18e1fb9cd9a049a3e6 from: namespace: openshift name: \u0026#39;nodejs:16-ubi8\u0026#39; lastTriggerTime: \u0026#39;2024-02-13T04:26:19Z\u0026#39; # listing buildconfigs $ oc get buildconfigs NAME TYPE FROM LATEST nodejs-ex-git Source Git 1 Autoscaling the nodejs-ex-git application # replacing resources: {} with following resources: limits: cpu: 30m memory: 100Mi requests: cpu: 3m memory: 40Mi # create HorizontalPodAudoscaler apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: nodejs-ex-git-hpa spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: nodejs-ex-git minReplicas: 1 maxReplicas: 3 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 10 # Increasing the load. $ for i in `seq 1000`; do curl -L https://nodejs-ex-git-sn-labs-x.labs-prod-openshift-san-a45631dc5778dc6371c67d206ba9ae5c-0000.us-east.containers.appdomain.cloud/; done Reference https://www.coursera.org/learn/ibm-containers-docker-kubernetes-openshift ","permalink":"http://localhost:1313/blogs/training/introduction-to-containers/introduction-to-red-hat-openshift/","summary":"Hands-on lab on Read Hat OpenShift","title":"Introduction to Red Hat OpenShift"},{"content":"Objective In this final project, I will build and deploy a simple guestbook application. The application consists of a web front end which will have a text input where you can enter any text and submit. For all of these I will create Kubernetes Deployments and Pods. Then I will apply Horizontal Pod Scaling to the Guestbook application and finally work on Rolling Updates and Rollbacks.\n# clone final project and modify it. $ git clone https://github.com/ibm-developer-skills-network/guestbook Lab Begins Review Criteria Task 1: Updation of the Dockerfile. (5 points) Hint:\nFROM instruction initializes a new build stage and specifies the base image that subsequent instructions will build upon. COPY command enables us to copy files to our image. ADD command is used to copy files/directories into a Docker image. RUN instruction executes commands. EXPOSE instruction exposes a particular port with a specified protocol inside a Docker Container. CMD instruction provides a default for executing a container, or in other words, an executable that should run in your container. # Updated Dockerfile FROM golang:1.15 as builder RUN go get github.com/codegangsta/negroni RUN go get github.com/gorilla/mux RUN go get github.com/xyproto/simpleredis/v2 COPY main.go . RUN go build main.go FROM ubuntu:18.04 COPY --from=builder /go//main /app/guestbook ADD public/index.html /app/public/index.html ADD public/script.js /app/public/script.js ADD public/style.css /app/public/style.css ADD public/jquery.min.js /app/public/jquery.min.js WORKDIR /app CMD [\u0026#34;./guestbook\u0026#34;] EXPOSE 3000 Task 2: The guestbook image being pushed to IBM Cloud Container Registry correctly. (1 point) # export for subsequent command. $ export MY_NAMESPACE=sn-labs-sgrraee # building guestbook app v1 $ docker build . -t us.icr.io/$MY_NAMESPACE/guestbook:v1 # pushing guestbook app to ICR $ docker push us.icr.io/$MY_NAMESPACE/guestbook:v1 # verify the image pushed successfully $ ibmcloud cr images Task 3: Index page of the deployed Guestbook – v1 application. (2 points) apiVersion: apps/v1 kind: Deployment metadata: name: guestbook labels: app: guestbook spec: replicas: 1 selector: matchLabels: app: guestbook strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: labels: app: guestbook spec: containers: - image: us.icr.io/sn-labs-sgrraee/guestbook:v1 imagePullPolicy: Always name: guestbook ports: - containerPort: 3000 name: http resources: limits: cpu: 50m requests: cpu: 20m # apply deployment $ kubectl apply -f deployment.yml deployment.apps/guestbook created ## port forward to view application kubectl port-forward deployment.apps/guestbook 3000:3000 Forwarding from 127.0.0.1:3000 -\u0026gt; 3000 Forwarding from [::1]:3000 -\u0026gt; 3000 Task 4: Horizontal Pod Autoscaler creation. (1 point) # autoscale guestbook deployment $ kubectl autoscale deployment guestbook --cpu-percent=5 --min=1 --max=10 horizontalpodautoscaler.autoscaling/guestbook autoscaled # check the status of newly-made HorizontalPodAutoscaler $ kubectl get hpa guestbook Task 5: The replicas in the Horizontal Pod Autoscaler being scaled correctly. (2 points) # generate load in new terminal $ kubectl run -i --tty load-generator --rm --image=busybox:1.36.0 --restart=Never -- /bin/sh -c \u0026#34;while sleep 0.01; do wget -q -O- \u0026lt;your app URL\u0026gt;; done\u0026#34; $ kubectl run -i --tty load-generator --rm --image=busybox:1.36.0 --restart=Never -- /bin/sh -c \u0026#34;while sleep 0.01; do wget -q -O- https://sgrraee-3000.theiaopenshiftnext-1-labs-prod-theiaopenshift-4-tor01.proxy.cognitiveclass.ai/; done\u0026#34; Task 6: The Docker build and push commmands for updating the guestbook. (2 points) # build and push $ docker build . -t us.icr.io/$MY_NAMESPACE/guestbook:v1 $ docker push us.icr.io/$MY_NAMESPACE/guestbook:v1 Task 7: Deployment configuration for autoscaling. (1 point) # update container resources resources: limits: cpu: 5m requests: cpu: 2m # apply the deployment changes $ kubectl apply -f deployment.yml deployment.apps/guestbook configured # port-forward $ kubectl port-forward deployment.apps/guestbook 3000:3000 # get the replicaset $ kubectl get rs NAME DESIRED CURRENT READY AGE guestbook-55df589d75 0 0 0 35m guestbook-bcccdbb4f 1 1 1 2m54s Task 8: Updated index page of the deployed Guestbook – v2 application after rollout of the deployment. (2 points) Task 9: The revision history for the deployment after rollout of the deployment. (2 points) # see history of deployment rollouts $ kubectl rollout history deployment/guestbook # see the details of deployment rollout $ kubectl rollout history deployments guestbook --revision=2 rev.webp\nTask 10: The updated deployment after Rollback of the update. (2 points) # get the replica sets and observe the deployment which is being used $ kubectl get rs NAME DESIRED CURRENT READY AGE guestbook-55df589d75 0 0 0 50m guestbook-bcccdbb4f 1 1 1 17m # rollout to revision 1 $ kubectl rollout undo deployment/guestbook --to-revision=1 deployment.apps/guestbook rolled back # get the replica sets after the rollout $ kubectl get rs Reference https://www.coursera.org/learn/ibm-containers-docker-kubernetes-openshift ","permalink":"http://localhost:1313/blogs/training/introduction-to-containers/build-and-deploy-a-simple-guestbook-app/","summary":"Final Hand-on Project to deploy simple guestbook app","title":"Build and Deploy a Simple Guestbook App"},{"content":"Case Study Presentation ","permalink":"http://localhost:1313/slides/card-skimming/","summary":"\u003ch2 id=\"case-study-presentation\"\u003eCase Study Presentation\u003c/h2\u003e\n\u003cdiv\n  style=\"padding-bottom:56.25%; position:relative; display:block; width: 100%\"\u003e\n  \u003ciframe\n    width=\"100%\"\n    height=\"100%\"\n    src=\"https://docs.google.com/presentation/d/e/2PACX-1vQN8GbQS70On-jFBa6hZgyl0olKliTRvT3ilwaUdmjfxVx1lda4tDPzBGIDb3WrEQ/embed?start=false\"\n    frameborder=\"0\"\n    scrolling=\"no\"\n    allowfullscreen=\"true\"\n    mozallowfullscreen=\"true\"\n    webkitallowfullscreen=\"true\"\n    style=\"position:absolute; top:0; left: 0\"\u003e\u003c/iframe\u003e\n\u003c/div\u003e","title":"Card Skimming"},{"content":"Introduction In this blog, I\u0026rsquo;ll be showing how we can perform cloud security auditing and assessment using ScoutSuite, Pacu and Prowler, and analyzing the report generated by these tools. I will demonstrate the privilege escalation attack on misconfigured policy of AWS.\nNote: The AWS access_key_id, access_secret_key used in the screenshot will not work and was only created for this writing this blog. If you want to test it, please make sure you are authorized to perform the assessment in targeted cloud.\nPrerequisites Configure AWS Amazon Web Services (AWS) is cloud computing services which will be a target cloud infrastructure for the assessment. To perform this activity, I created AWS account from my own account. To continue follow the given instructions:\nNavigate to Identity Access Management (IAM) Service\nGo to Users \u0026gt; Create user to create a user named kaliuser\nNext, Go to User groups \u0026gt; Create group to create a user group testing.\nAdd the kaliuser in a group while creating. Attach Policy SecurityAudit and SecurityAudit to providing permissions. Then, navigate to User \u0026gt; kaliuser \u0026gt; Security credentials \u0026gt; Access keys \u0026gt; Create access key \u0026gt; Select Command Line Interface (CLI) \u0026gt; Next \u0026gt; Create access key. This will generate programmatic access key and secret key.\nFinally, configure your shell running aws configure, enter the access_key_id and access_secret_key in the terminal.\n$ aws configure AWS Access Key ID [****************TKIZ]: AWS Secret Access Key [****************/G0w]: Default region name [us-east-2]: Default output format [None]: Note: I will further configure additional Policy for Pacu in next section.\nScoutSuite Scout Suite is an open source multi-cloud security-auditing tool, which enables security posture assessment of cloud environments.\ngit clone git@github.com:nccgroup/ScoutSuite.git virtualenv -p python3 venv # Create virtual environment source venv/bin/activate # Activate virtual environment in shell cd ScoutSuite pip install # Install dependencies Setup Pacu Pacu is an open-source AWS exploitation framework, designed for offensive security testing against cloud environments\ngit clone git@github.com:RhinoSecurityLabs/pacu.git source venv/bin/activate # Activate virtual environment in shell cd pacu ./install.sh # Install Dependencies Setup Prowler Prowler is an Open Source security tool to perform AWS, GCP and Azure security best practices assessments, audits, incident response, continuous monitoring, hardening and forensics readiness.\ngit clone git@github.com:prowler-cloud/prowler.git source venv/bin/activate # Activate virtual environment in shell cd prowler pip install prowler # Install prowler and its dependencies ./prowler.py aws -f us-east-2 -s {s3,iam,cloudtrail,cloudwatch} # Running ScoutSuite Assessment Navigate to cloned folder, activate the environment and run the scout suite.\ncd ScoutSuite python3 scout.py # Start auditing using scout.py Running Scout Suite will take some minutes to complete.\nAfter completion, you\u0026rsquo;ll see html file as an output stored inside scoutsuite-report folder. Go to the folder and open it in your browser.\nCloudwatch Recommendation Here, CloudTrail is not configured in dashboard signifies it is a crucial issue and vulnerability as AWS CloudTrail plays a critical role in monitoring and auditing AWS resources and API activities. Without it being configured, we will lack visibility into actions taken within AWS environment, making it difficult to detect and investigate security incidents.\nTo address this, we should create a CloudTrail trail to log events in all AWS Regions. This trail should deliver log files to an Amazon S3 bucket which is storage service of AWS. As per the best practices documentation, we also need to enable CloudTrail log file integrity and integrate it with Amazon CloudWatch Logs for monitoring to make sure the file is not changed. Additionally, it is considered good practice to use AWS Security Hub to monitor CloudTrail resources.\nIAM Recommendation In general, it shows two different types of vulnerabilities:\nPassword Policies: The vulnerabilities related to inadequate password length, expiration, and reuse signify a weak password policy in your AWS environment.\nTo remediate these issues, we need to enforce stronger password policies that require longer passwords, implement regular password expiration intervals (e.g., 90 days), and disallow password reuse to enhance security.\nRoot Account Security: The vulnerabilities regarding root account usage and the absence of Multi-Factor Authentication (MFA) highlight the need for securing the root AWS account.\nTo address these issues, we need to ensure that IAM policies are attach only the users and groups, enable hardware-based MFA for the root account, and enforce MFA for all users accessing the AWS environment. Additionally, limit the use of the root account and establish individual IAM user accounts with appropriate permissions to reduce security risks associated with the root account.\nPacu Exploitation In Pacu, I will try to escalate the privilege by using the tool command. To do this, I have to add custom policy referred as misconfigured customer managed policy. Practically, it is possible someone can add hazardous policy without knowing the consequences and human error or misconfiguration can happen. To do this follow these steps:\nAdding custom policy Navigate to Identity Access Management (IAM) Service\nGo to Policies \u0026gt; Create policy \u0026gt; JSON to create new policy PacuExploitPolicy.\nCopy and paste following lines in Policy Editor \u0026gt; Next \u0026gt; Create Policy.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor1\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::cybr-pacu-lab-example\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;Statement1\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;iam:Get*\u0026#34;, \u0026#34;iam:List*\u0026#34;, \u0026#34;iam:Put*\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:SimulateCustomPolicy\u0026#34;, \u0026#34;iam:SimulatePrincipalPolicy\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Source: intro to AWS PENTESTING (with Pacu)\nNext, attach the custom policy to the testing group or kaliuser user directly. I attached for user group.\nNote: In the custom policy, iam:Put* is risky attribute which allow to add new Policies leading to escalate the policy. You\u0026rsquo;ll see how that can be achieved in the next section.\nRunning Pacu cd pacu ./cli.sh # Start Pacu. Setting up AWS credentials First, we need to setup the AWS keys using set_keys and set_regions command.\nBegin Exploitation Before beginning exploitation, you can enumerate the IAM permissions using command run iam__enum_permissions. Now, I run command run iam__privesc_scan, you\u0026rsquo;ll see output something like this:\nYou can see escalation method PutGroupPolicy and PutUserPolicy is used to attempt the privilege escalation. In the highlighted section, new administrator policy to the current user has been added with policy named o7go7jwhnq.\nYou can verify by running run iam__enum_permissions command in pacu terminal where you\u0026rsquo;ll notice, objects with escalated admin permissions.\nTakeaways Pacu provides several commands for pen testers to enumerate, privilege escalate, reconnaissance, exfiltration, exploitation, and persistence on the given AWS account. In this activity, I used iam__privesc_scan which is a command designed to scan for and exploit privilege escalation vulnerabilities in AWS Identity and Access Management (IAM) policies. If the policy is weak or vulnerable, this command can look for multiple approach to escalate privilege to a current AWS user. This command can be used in penetration testing for following purposes:\nIdentify Weak IAM Policies that can be potentially exploited for privilege escalation. Demonstrate impact of weak policies by creating, modifying, and deleting AWS resources. Assessing the Security Posture is a great advantage as it can help identify and highlight areas where IAM policies need to be improved. Prowler Assessment cd prowler ./prowler.py aws -f us-east-2 -s {s3,iam,cloudtrail,cloudwatch} # Run Prowler Here, I used prowler to scan for specific services like S3, IAM, Cloudtrail, Cloudwatch.\nThe output is summarized in the terminal or we can open the html file for detailed results in browser.\nFrom the Prowler result the custom policy that we created is identified as vulnerable with high severity:\nSeverity: High Service Name: IAM (Identity and Access Management) Region: us-east-2 Check ID: iam_policy_allows_privilege_escalation Check Title: Ensure no Customer Managed IAM policies allow actions that may lead to Privilege Escalation Resource ID: Custom Policy arn:aws:iam::201368012826:policy/PacuExploitPolicy allows privilege escalation using the following actions: {\u0026lsquo;iam:AttachRolePolicy\u0026rsquo;}. Status: FAIL Risk: Users with some IAM permissions are allowed to elevate their privileges up to administrator rights. Associated with: MITRE ATT\u0026amp;CK and AWS Well-Architected Framework (Security Pillar) Recommendations: Granting usage permission on a per-resource basis Applying the least privilege principle. The best practice is to review and adjust the polices to remove the risky permissions. Regular auditing and assessment of IAM policies is also important to figure out the security posture. Conclusion We covered three powerful open-source cloud assessment tools - ScoutSuite, Pacu and Prowler.\nScoutsuite allowed us to perform a comprehensive scan of AWS, identifying vulnerabilities, misconfigurations, and potential security risks. Pacu enabling us to simulate attacks and assess the security of our AWS environment from an attacker\u0026rsquo;s perspective Prowler with more robust assessment by conducting security best practice checks and compliance checks based on industry standards. Furthermore, I demonstrated how a simple misconfiguration could be exploited using these Pacu to gain higher privilege access. By regularly conducting assessments and adhering to best practices when creating policies and configurations, we can improve our defenses and minimize the potential for such cloud security breaches.\nResources AWS Pentesting (with pacu) by Cybr ","permalink":"http://localhost:1313/blogs/cybersecurity/cloud-security-assessment-using-scoutsuite-pacu-prowler/","summary":"Perform Cloud Security Assessment, analyzing the report and providing recommendation.","title":"Cloud Security Assessment using ScoutSuite, Pacu, and Prowler"},{"content":"Key Achievements Event Excellence: Achieved recognition through outstanding performance in events, cybersecurity awareness presentation and competitions. Peer Mentor: Respected by peers for knowledge sharing and creating a supportive learning environment. Proactive Leader: Known for driving positive change and continuous improvement. Gallery Announcement Rewarded Certificate ","permalink":"http://localhost:1313/honors/student-of-the-month-october-2023/","summary":"Awarded as student of the October Month","title":"Student of the Month October 2023"},{"content":"Objective Amazon VPC is web service that allows us to create a private network in the cloud. In this lab,\nPart 1: Deploy VPC with public subnets Part 2: Deploy an EC2 instance into VPC Lab Begins Part 1: Exploration VPC Amazon Virtual Private Cloud is a virtual network that is dedicated to your AWS account\nNote: The VPC IPv4 CIDR is 172.31.0.0/16\nSubnets A subnet is a sub-range of IP addresses in the VPC.\nNote: Auto-assign public IPv4 address is Yes. The Subnet IPv4 CIDR is 172.31.0.0/20. Since, AWS reserves five addresses in each subnet for it\u0026rsquo;s own purpose such that only 4091 address are available out of 4096.\nInternet Gateway An internet gateway allows communication between the resources in a VPC and the internet.\nRoute Table A route table contains a set of rules, called routes, that are used to determine where network traffic is directed.\nNote: 172.31.0.0/16 is routed locally, allowing all subnets in a VPC to communicate with each other and all public traffic (0.0.0.0/0) is routed to the internet gateway.\nSecurity Group A security group acts as a virtual firewall for instances to control inbound and outbound traffic.\nInbound Rules Outbound Rules Custom Security Group Inbound Rules EC2 instance\u0026rsquo;s VPC and Subnet Exploring EC2 Instance Navigating Public IP Part 2: Creation Custom VPC VPC CIDR VPC Subnets VPC Preview VPC Create VPC Page Navigate Custom Security Group Security Group Create Security Group Page Navigate EC2 Instance in Custom VPC EC2 AMI EC2 Network Setting EC2 User Data The End\n","permalink":"http://localhost:1313/blogs/training/amazon-vpc-lab/","summary":"Explore and create custom VPC and deploy EC2 instance in VPC","title":"Amazon Virtual Private Cloud (VPC) Lab"},{"content":"Working with AWS CodeCommit (Fundamental) Overview - Lab 1 AWS CodeCommit is a highly scalable, managed source control service that hosts private Git repositories. CodeCommit stores your data in Amazon S3 and Amazon DynamoDB giving your repositories high scalability, availability, and durability. You simply create a repository to store your code.\nThis lab demonstrates how to:\nCreate a code repository using AWS CodeCommit via the Amazon Management Console Create a local code repository on the Linux instance running in EC2 using git Synchronize a local repository with an AWS CodeCommit repository Lab 1 Begins Creating an AWS CodeCommit repository Connecting to AWS EC2 Instance using AWS Session Manager Visualizing the instance session in AWS Session Manager Working on Linux local repository in Console Connection Configuring the Git credential helper with AWS credential profile, which allows Git credential helper to send the path to repositories.\ngit config --global credential.helper \u0026#39;!aws codecommit credential-helper $@\u0026#39; git config --global credential.UseHttpPath true git clone https://git-codecommit.us-east-1.amazonaws.com/v1/repos/My-Repo. Pushed Changes in AWS CodeCommit Repository Auditing Your Security with AWS Trusted Advisor (Intermediate) Overview - Lab 2 This lab guides you through the steps to audit your AWS resources to ensure your configuration complies with basic security best practices. This lab makes use of AWS Trusted Advisor as it applies to security. The topics covered include working with security groups, Multi-factor Authentication (MFA), and AWS Identity and Access Management (IAM).\nNote: AWS Security Model is shared responsibility, where both AWS and customers come together to achieve security objective. AWS provides controls that customer can use to secure cloud resources includes IAM, Amazon Virtual Private Clouds (VPCs), security groups, network ACLs and certificates.\nYou will be able to do the following:\nUse Trusted Advisor to perform a basic audit of your AWS resources Modify Amazon Elastic Compute Cloud (Amazon EC2) Security Groups to meet best practices Configure Multi-factor Authentication (MFA) (Optional, requiring installation of software on a mobile device) Lab 2 Begins Check Trusted Advisor Recommendations Security Group with unrestricted ports Editing inbound rules Editing TCP inbound rules The port (tcp/port 21) is unrestricted in the security group and is not currently needed and should be removed from the rules.\nModified FTP port inbound rule Editing MySQL inbound rules The rule is permitting incoming traffic to port 3306 from 0.0.0.0/0, which means traffic will be permitted from any computer on the Internet.\nModified MySQL inbound rules This rule now permits access to the RDS database only from members of the Web Security Group\nRe-check Trusted Advisor Recommendations Result In some situations, you want to approve having ports open like tcp/22 and tcp/3389 this can be simple done by selecting Exclude \u0026amp; Refresh.\nSetting MFA in AWS IAM user Automate Application Testing Using AWS CodeBuild (Advanced) Overview - Lab 3 Incorporating automated testing into your DevOps pipelines is crucial to increase speed and efficiency by ensuring that your application functions properly after every update.\nThis lab demonstrates how you can use AWS CodeBuild as a part of your Continuous Integration pipelines to test and build your code. You will be able to:\nConfigure CodeBuild to perform application testing Troubleshoot and fix CI/CD pipeline failures Review CodeBuild reports and logs Apply common code testing strategies Describe the importance of robust test coverage Lab 3 Begins Explore application in AWS Cloud9 Browsing Cloud9 Cloud9 Development Previewing Application in Cloud9 Automating Testing in CodeBuild The test result and failures will not be visible or easy to interpret, CodeBuild support to display test and coverage reports exported by framework in the reports section of buildspec.yml. CodeBuild supports the following formats: Cucumber JSON, JUnit XML, NUnit XML, NUnit3 XML, TestNG XML, and Visual Studio TRX. For code coverage reports, you can use the following formats: JaCoCo XML, SimpleCov JSON, Clover XML, and Cobertura XML.\nAdding CI step and redirecting to reports Pushing changes to CodeCommit Review Build Pipeline Failed CodePipeline of Web Application Command Execution Error Pushing the fix detected by Automated Tests Error: This build was not successful due to following error.\n/codebuild/output/src2610253420/src/react-app/node_modules/@adobe/ css-tools/dist/index.cjs:118 }, options?.source || \u0026#34;\u0026#34;); ^ SyntaxError: Unexpected token \u0026#39;.\u0026#39; at Runtime.createScriptFromCode (node_modules/jest-runtime/build/index.js:1350:14) at Object.\u0026lt;anonymous\u0026gt; (node_modules/@testing-library/jest-dom/dist/utils.js:21:17) ","permalink":"http://localhost:1313/blogs/training/modern-devops-in-the-aws-console/","summary":"AWS Builder Labs in AWS Console","title":"Learn Modern DevOps in the AWS Console"},{"content":"Expressing Gratitude I am thrilled and honored to share that I have won the inter-college CTF Competition sponsored by EC-Council, held at Sault College, TSW and BSW Campuses. I am grateful for the prize and recognition that I received from EC-Council, and were presented to me by the campus director of Sault College, Toronto South West.\nA special mention must be made of my instructor and program coordinator, who has consistently been a source of inspiration and played a key role in my achievement.\nAbout Competition The challenge was performed on CyberQ, a cloud-based platform that provides cyber targets for participants to capture the flag. It was individual competition, where each participant had to complete a scenario-based challenge within one hour, with three hints available in each flag at the cost of reduced score.\nThe competition involved red teaming, individual participants, who performed cyber attacks simulation on target system on a target network and the white team, EC-Council’s technical team, who monitored and evaluated the performance of the participants.\nI\u0026rsquo;ve written technical walkthrough explaining my preparation process, knowledge I gained and the strategy I employed to successfully capture all flags in just 38 minutes. You can find it here 👈.\nCaptured Moments Competition Begun Prizes Drawn Swags On Stickers Adorn ","permalink":"http://localhost:1313/honors/winner-of-capture-the-flag-competition/","summary":"Experience sharing of CTF Competition sponsored by EC-Council","title":"Winner of Capture The Flag Competition"},{"content":" Important Note: Always follow ethical guidelines. Never use these skills for illegal activities.\nYou can find competition galleries here 👈.\nFamiliarization The preparation journey led me to platforms like Hack The Box, TryHackMe, and CyberQ, all of which offered a free tier Capture the Flag resources and challenges, it helped me get started.\nI began solving basic challenges and gradually familiarized with the platforms, gaining fundamental skills in port scanning and network discovery done for enumeration as part of the reconnaissance process. As I progressed, I gained knowledge about potential network port specific misconfigurations that could potentially bypass authentication. In most challenges, password cracking, directory traversal, reverse shell and achieving privilege escalation were intermediate steps crucial for capturing the flags. I realized the importance of becoming proficient in these concepts in ethical hacking.\nUnderstanding all the concepts and tools was challenging, so I began to compile reference notes. This not only helped me memorize but also served as a handy guide. You can refer here 👈.\nBuilding Mindset CTF challenges and competitions can vary based on different cloud platforms. Here are the basic strategy to help you start building a mindset for cracking CTFs:\nEnumeration Discover networks to find the IP address of active hosts in the given network. Scan for operating system, open service ports with versions and look for common misconfigurations that could lead to bypass the authentication process. Use a browser to explore the application or perform directory traversal to find unsecured hidden paths. Exploitation If password locking is an issue, there are many ways to crack passwords using dictionary or brute force attacks. Depending on the situation and target platform, decide which tools will be most effective for cracking password. Familiarize with the concept of reverse shell and tools that can generate the reverse shell file. This file will help us redirect to the target computer\u0026rsquo;s shell, granting remote access. This file can be injected either after connecting through open ports or from cracked password. Once attacker get into the target system, there\u0026rsquo;s a likelihood of further more misconfigurations. These can allow attacker to perform privilege escalation, potentially granting elevated access to resources. Competition Context We were given a scenario with instructions and vulnerable cloud system in EC-Council\u0026rsquo;s CyberQ platform. There were seven flags in total to capture, with the final one being the most challenging. Each flag was linked to the previous one, either directly or indirectly, and the seventh flag couldn’t be captured without finding the first six. In this challenge, we had to demonstrate scanning network skills, password cracking skills, reverse shell and privilege escalation in a Linux based target system.\nThe Pain Unfortunately, many of us faced difficulties as the virtual machine (VM) provided for the competition was hosted on a server in Singapore. Accessing it from North America resulted in a slow server response, keyboard input lag, and key bouncing. Despite the intense competitive environment and the server frustration, I successfully captured all the flags in just 38 minutes.\nAnswer Hint Pattern Description Example N, NN Represents single or two digit numeric 6, 22 aaaaaa Represents all small alphabet of length 6 hannaa aaaaaaaaaa-aaaaa-aaaaaa Represents small alphabets with exact length and hyphen in between gracemedia-media-player NANNNNaaN.NaaNNaN-NNaNaNaNNNNNNNAN Represents combination of alphabets and numbers 1F6154fc2.7fa29a5-48c6a7b6902590E7 Tips CyberQ Tip: To overcome the key bouncing issue, I typed commands in my system’s notepad and then copied them to the CyberQ VM using the Paste Clipboard Text button.\nTroubleshooting Tip: Reset your VM, if you face black screen of death or any OS issue.\nSubmission Tip: It is specific to CyberQ, even if you submit all your flags correctly, the time doesn\u0026rsquo;t stop unless you Ends/Terminates the running by the VM using the red power button. Then, finish lab by clicking Finish button.\n","permalink":"http://localhost:1313/blogs/cybersecurity/ctf-competition-preparation-journey/","summary":"Experience sharing, Cracking CTF preparation for competition","title":"Capture The Flag Competition Preparation Journey"},{"content":"In this step-by-step guide, I\u0026rsquo;ll walk you through the process of setting up your personal blog site using Hugo, a popular static site generator written in Go, and the PaperMod theme.\nAlong with that, I\u0026rsquo;ll also show you how I configured my domain and hosting for this site. If you are following then by the end of this tutorial, you\u0026rsquo;ll have your blog up and running and ready to share your thoughts with the world.\nStep 0: Identify Purpose I opted for a static website generator because I wanted a website that\u0026rsquo;s incredibly easy to maintain and configure. Additionally, it had to be fast and secure. My primary goal is to have a personal site where I can centralize all my contents and showcase my work, all in one place. I had straightforward requirements, which led me to choose a static site generator.\nStep 1: Static Site Generator with hugo Comparing Gatsby vs Hugo, \u0026ldquo;Generated websites are super fast\u0026rdquo; vs \u0026ldquo;Lightning fast\u0026rdquo;. According to my personal website idea, it doesn\u0026rsquo;t require a dynamic contents or any user interactions, which is purely based on static architecture. I selected Hugo over Gatsby.\nInstall Go, Hugo The first step is to install Go and Hugo on your local machine. You can follow the installation instructions for your specific operating system here.\nPlease refer to the relevant documentation for installation instructions:\nGit Go Dart Sass Create a New Hugo Site Once Hugo is installed, create a new Hugo site using the following command:\nhugo new site sagarchamling.com Replace sagarchamling.com with the desired name of your blog.\nStep 2: Choosing Theme Hugo offers a wide range of themes. Find your choice and make sure you give them a credit.\nInstall the PaperMod Theme For my blog, I\u0026rsquo;ll use the PaperMod theme. To install it, navigate to your theme\u0026rsquo;s github page here and run:\ngit init # Initialize your git repository git submodule add --depth=1 https://github.com/adityatelange/hugo-PaperMod.git themes/PaperMod Next, open your blog\u0026rsquo;s config.toml file and set the theme parameter to \u0026quot;PaperMod\u0026quot;. or You can take a reference of exampleSite to begin with.\nStep 3: Customize Your Site Customizing theme css .(site root) ├── content/ ├── theme/PaperMod/ └── assets/ └── css/ └── extended/ \u0026lt;--- ├── custom.css \u0026lt;--- └── any_name.css \u0026lt;--- Customizing header and footer partials .(site root) ├── config.yml ├── content/ ├── theme/hugo-PaperMod/ └── layouts ├── partials │ ├── extend_footer.html \u0026lt;--- │ └── extend_head.html \u0026lt;--- └── robots.txt Create Content Create your blog posts in the content directory. You can use Markdown to format your posts. For example, to create a new post, run:\nhugo new blogs/my-first-post.md Edit the Markdown file to write your content.\nStep 4: Deploy in Github Pages Create a .gitignore file Create a .gitignore file to exclude unnecessary files from version control. Here\u0026rsquo;s a sample .gitignore for Hugo sites:\n/resources/ /public/ Commit and Push to GitHub Commit your changes and push your blog to a GitHub repository:\ngit add . git commit -m \u0026#34;Initial commit\u0026#34; git remote add origin git@github.com/cham11ng/sagarchamling.com git push -u origin main Step 5: Select Domain Name I considered using a domain registrar like Porkbun for two major reason; it\u0026rsquo;s cheap and easy configuration process.\nSearch for available domain names, and once you find one that suits your blog, register it.\nStep 6: Configure and Publish Site Configure Domain In your domain registrar\u0026rsquo;s dashboard (Porkbun in this case), configure your domain\u0026rsquo;s DNS settings. This typically involves setting up a CNAME record with the GitHub Pages URL provided earlier.\nGo to the DNS Records section and select \u0026ldquo;Edit.\u0026rdquo;\nIn the Quick DNS Config section, select the \u0026ldquo;Github\u0026rdquo; button to quickly configure domain\u0026rsquo;s DNS settings to point Github Pages.\nYou\u0026rsquo;ll also need to create your subdomain CNAME record. In the Host field, enter the subdomain you want. It can be \u0026ldquo;www\u0026rdquo; or whatever you wish it to be.\nFinally, you\u0026rsquo;ll see your DNS records successfully updated to the Github Pages DNS under Current Records.\nConfigure GitHub Pages In your GitHub repository, go to the Settings tab and scroll down to the Pages section. Provide the custom domain with a URL for your published site. It may take some time for DNS changes to propagate. Once it\u0026rsquo;s done, your blog will be accessible through your custom domain.\nCongratulations! You\u0026rsquo;ve successfully set up your personal blog site using Hugo, customized it with the PaperMod theme, and published it with your own domain name. Now, you\u0026rsquo;re ready to start sharing your thoughts and ideas with the world. Happy blogging!\n","permalink":"http://localhost:1313/blogs/tech/how-i-setup-my-website-using-hugo-github-pages-and-porkbun/","summary":"Step by Step Guide for setting up a personal website","title":"How I setup my website using Hugo, Github Pages and Porkbun"},{"content":"20th Years of Cybersecurity Awareness Program Participation Certificate ","permalink":"http://localhost:1313/slides/phishing-cybersecurity-awareness-2023/","summary":"Presentation on Phishing during Cybersecurity Awareness 2023","title":"Phishing - A Social Engineering Attack"},{"content":"Linux Final Presentation ","permalink":"http://localhost:1313/slides/linux-essentials-commands-used-by-system-administrator/","summary":"Presentation on Linux System Administrator commands","title":"Linux Essentials Commands Used by System Administrator"},{"content":"About Lab Cisco Generic Routing Encapsulation (GRE) is a tunneling protocol that provides a simple generic approach to transport packets of one protocol over another protocol by means of encapsulation. In this blog, I will demonstrate how to configure GRE tunnel using given topology and addressing scheme given below.\nNetwork Topology Addressing Scheme Device Interface IP Address Subnet Mask Default Gateway RA-SAGAR G0/0 192.168.1.1 255.255.255.0 N/A S0/0/0 64.103.211.2 255.255.255.252 N/A Tunnel 0 10.10.10.1 255.255.255.252 N/A RB-SAGAR G0/0 192.168.2.1 255.255.255.0 N/A S0/0/0 209.165.122.2 255.255.255.252 N/A Tunnel 0 10.10.10.2 255.255.255.252 N/A RC-SAGAR S0/0/0 64.103.211.1 255.255.255.252 N/A S0/0/1 209.165.122.1 255.255.255.252 N/A SA-CHAMLING VLAN 1 192.168.1.2 255.255.255.0 192.168.1.1 SB-CHAMLING VLAN 1 192.168.2.2 255.255.255.0 192.168.2.1 PC-A NIC 192.168.1.11 255.255.255.0 192.168.1.1 PC-C NIC 192.168.2.22 255.255.255.0 192.168.2.1 Implementation Required Resources 2 Switches (Cisco 2960) 3 Routers (Cisco 1941) 2 PCs (Windows with Terminal Emulation Program) Cables: Console cables to configure the Cisco IOS devices through the console port Ethernet cables as shown in the topology Configuring Switches Configuring with sets of commands in Switch SA\nSwitch\u0026gt; en Switch# config t Switch(config)# hostname SA-CHAMLING SA-CHAMLING(config)# hostname SA-CHAMLING SA-CHAMLING(config)# no ip domain-lookup SA-CHAMLING(config)# banner motd #Unauthorized access is strictly prohibited.# SA-CHAMLING(config)# int vlan 1 SA-CHAMLING(config-if)# ip address 192.168.1.2 255.255.255.0 SA-CHAMLING(config-if)# no shut SA-CHAMLING(config-if)# exit SA-CHAMLING(config)# ip default-gateway 192.168.1.1 SA-CHAMLING(config)# end SA-CHAMLING#copy run start Configuring with sets of commands in Switch SB\nSwitch\u0026gt; en Switch# config t Switch(config)# hostname SB-CHAMLING SA-CHAMLING(config)# no ip domain-lookup SA-CHAMLING(config)# banner motd #Unauthorized access is strictly prohibited.# SA-CHAMLING(config)# int vlan 1 SA-CHAMLING(config-if)# ip address 192.168.2.2 255.255.255.0 SA-CHAMLING(config-if)# no shut SA-CHAMLING(config-if)# exit SA-CHAMLING(config)# ip default-gateway 192.168.2.1 SA-CHAMLING(config)# end SA-CHAMLING# copy run start Configuring Routers Configuring with sets of commands in Router RA\nen config t hostname RA-SAGAR no ip domain-lookup banner motd #Unauthorized access is strictly prohibited.# interface g0/0 ip address 192.168.1.1 255.255.255.0 no shut interface s0/0/0 ip address 64.103.211.2 255.255.255.252 no shut end copy run start Configuring with sets of commands in Router RB\nen config t hostname RB-SAGAR no ip domain-lookup banner motd #Unauthorized access is strictly prohibited.# interface g0/0 ip address 192.168.2.1 255.255.255.0 no shut interface s0/0/0 ip address 209.165.122.2 255.255.255.252 no shut end copy run start Configuring with sets of commands in Router RC\nen config t hostname RC-SAGAR no ip domain-lookup banner motd #Unauthorized access is strictly prohibited.# interface s0/0/0 ip address 64.103.211.1 255.255.255.252 clock rate 2000000 no shut interface s0/0/1 ip address 209.165.122.1 255.255.255.252 clock rate 2000000 no shut end copy run start Configuring Static Route Since, the routers (RA and RB) is connected with R3, it needs to know how to reach to the users connected to the other end router. So, we need to configure static route.\nRA-SAGAR# config t RA-SAGAR(config)# ip route 192.168.2.0 255.255.255.0 10.10.10.2 RB-SAGAR# config t RB-SAGAR(config)# ip route 192.168.1.0 255.255.255.0 10.10.10.1 Configuring GRE Tunnel Configuring tunnel mode gre in router RA\ninterface tunnel 0 ip address 10.10.10.1 255.255.255.252 tunnel source s0/0/0 tunnel destination 209.165.122.2 tunnel mode gre ip no shutdown end copy run start Configuring tunnel mode gre in router RB\ninterface tunnel 0 ip address 10.10.10.2 255.255.255.252 tunnel source s0/0/0 tunnel destination 64.103.211.2 tunnel mode gre ip no shutdown end copy run start Configuring PCs Configuring PCA Configuring PCB Testing Verifying tunnel interfaces in Router RA and RB. Successful ping and tracert from PCA to PCB. Successful ping and tracert from PCA to PCB Download .PKT file ","permalink":"http://localhost:1313/blogs/networking/configuring-gre-tunnel-using-cisco-packet-tracer/","summary":"Setup GRE Tunnelling to establish connection","title":"Configuring GRE Tunnel using Cisco Packet Tracer"},{"content":"Subnetting Subnetting is defined as breaking down of huge network address to manageable networks. For the subnet addressing scheme to work, every host on the network must know which part of the host address will be used as the subnet address. The table gives the default subnet mask of each class.\nCIDR notation indicates the network mask for an address and adds on the total number of turned on bits in the entire address using slash notation.\nDefault Subnet Mask Class Format Default Subnet Mask CIDR A network.host.host.host 255.0.0.0 /8 B network.network.host.host 255.255.0.0 /16 C network.network.network.host 255.255.255.0 /24 Subnetting Formula How many subnets? 2x = number of subnets. x is the number of masked bits or turned on network bit.\nHow many hosts per subnet? 2y – 2 = number of hosts per subnet. y is the number of unmasked bits or turned off host bit.\nWhat are the valid subnets? 256 – subnet mask = block size\nWhat’s the broadcast address for each subnet? The number right before the value of the next subnet.\nSubnet 0 128 First host 1 129 Last host 126 254 Broadcast Address 127 255 Real-world Problem Scenario I 192.168.1.0/24 network address is assigned to do subnetting for the topology seen below.\nExplanation I Total required networks include:\n2 LANs individually connected with the highest number of connected hosts, that is 2. 1 WAN between routers Given network address is 192.168.1.0/24 which belongs to Class C with default subnet mask 255.255.255.0 If we subnet with /26 i.e., 255.255.255.192, then block size is: 256 – 192 = 64 Subnetwork: 22 = 4 Valid Hosts per subnetwork 26 - 2 = 62 Answer I How many subnetworks can be seen as requirements in the figure above? 3 How many maximum hosts are required in any subnetwork? 3 How many bits will be borrowed from hosts portion in order to accommodate the required number of subnets? 2 How many subnetworks will be created by borrowing the bits as mentioned above? 22 = 4 How many subnetworks will remain unused for future use? 4 – 3 = 1 How many valid host addresses will be available per subnetwork? 26 - 2 = 62 After changing the length/CIDR of original network, what is the new subnet mask? 255.255.255.192 Fill in the following table with the subnetwork’s information. Subnetwork # Subnetwork Address First Valid Host Last Valid Host Broadcast Address 1 192.168.1.0 192.168.1.1 192.168.1.62 192.168.1.63 2 192.168.1.64 192.168.1.65 192.168.1.126 192.168.1.127 3 192.168.1.128 192.168.1.129 192.168.1.190 192.168.1.191 4 192.168.1.192 192.168.1.193 192.168.1.254 192.168.1.255 Scenario II 192.168.20.0/24 network address is assigned to you to do subnetting for the topology seen below.\nExplanation II Total required networks include:\n4 LANs individually connected with the highest number of connected hosts, that is 3. 4 WAN between routers Given network address is 192.168.20.0/24 which belongs to Class C with default subnet mask 255.255.255.0 If we subnet with /27 i.e., 255.255.255.224, then block size: 256 – 192 = 64 Subnetwork: 23 = 8 Valid Hosts per subnetwork 25 - 2 = 30 Answer II How many subnetworks can be seen as requirements in the figure seen above? 8 How many maximum hosts are required in any subnetwork? 3 How many bits will be borrowed from hosts portion in order to accommodate the required number of subnets? 3 How many subnetworks will be created by borrowing the bits as mentioned above? 23 = 8 How many subnetworks will remain unused for future use? 8 – 8 = 0 How many valid host addresses will be available per subnetwork? 25 - 2 = 30 After changing the length/CIDR of original network, what is the new subnet mask? 255.255.255.224 Fill in the following table with the subnetwork’s information. Subnetwork # Subnetwork Address First Valid Host Last Valid Host Broadcast Address 1 192.168.20.0 192.168.20.1 192.168.20.30 192.168.20.31 2 192.168.20.32 192.168.20.33 192.168.20.62 192.168.20.63 3 192.168.20.64 192.168.20.65 192.168.20.94 192.168.20.95 4 192.168.20.96 192.168.20.97 192.168.20.126 192.168.20.127 5 192.168.20.128 192.168.20.129 192.168.20.158 192.168.20.159 6 192.168.20.160 192.168.20.161 192.168.20.190 192.168.20.191 7 192.168.20.192 192.168.20.193 192.168.20.222 192.168.20.223 8 192.168.20.224 192.168.20.225 192.168.20.254 192.168.20.255 Scenario III 172.16.0.0/16 network address is assigned to you to do subnetting for the topology seen below.\nExplanation III Total required networks include:\n9 LANs individually connected with the highest number of connected hosts, that is 3900. 6 WANs between routers. 4 Given network address is 172.16.0.0/16 which belongs to Class B with default subnet mask 255.255.0.0 If we subnet with /20 i.e., 255.255.240.0, then the block size: 256 – 240 = 16 Subnetwork: 24 = 16 Valid hosts per subnetwork 212 - 2 = 4094 Answer III How many subnetworks can be seen as requirements in the figure seen above? 15 How many maximum hosts are required in any subnetwork? 3900 How many bits will be borrowed from the host portion in order to accommodate the required number of subnets? 4 How many subnetworks will be created by borrowing the bits as mentioned above? 24 = 16 How many subnetworks will remain unused for future use? 16 – 15 = 1 How many valid host addresses will be available per subnetwork? 212 - 2 = 4094 After changing the length/CIDR of original network, what is the new subnet mask? 255.255.240.0 Fill in the following table with the subnetwork’s information. Subnetwork # Subnetwork Address First Valid Host Last Valid Host Broadcast Address 1 172.16.0.0 172.16.0.1 172.16.15.254 172.16.15.255 2 172.16.16.0 172.16.16.1 172.16.31.254 172.16.31.255 3 172.16.32.0 172.16.32.1 172.16.47.254 172.16.47.255 4 172.16.48.0 172.16.48.1 172.16.63.254 172.16.63.255 5 172.16.64.0 172.16.64.1 172.16.79.254 172.16.79.255 6 172.16.80.0 172.16.80.1 172.16.95.254 172.16.95.255 7 172.16.96.0 172.16.96.1 172.16.111.254 172.16.111.255 8 172.16.112.0 172.16.112.1 172.16.127.254 172.16.127.255 9 172.16.128.0 172.16.128.1 172.16.143.254 172.16.143.255 10 172.16.144.0 172.16.144.1 172.16.159.254 172.16.159.255 11 172.16.160.0 172.16.160.1 172.16.175.254 172.16.175.255 12 172.16.176.0 172.16.176.1 172.16.191.254 172.16.191.255 13 172.16.192.0 172.16.192.1 172.16.207.254 172.16.207.255 14 172.16.208.0 172.16.208.1 172.16.223.254 172.16.223.255 15 172.16.224.0 172.16.224.1 172.16.239.254 172.16.239.255 16 172.16.240.0 172.16.240.1 172.16.255.254 172.16.255.255 ","permalink":"http://localhost:1313/blogs/networking/understanding-subnetting-to-desing-addressing-scheme/","summary":"Solving a subnet problem of given network diagrams.","title":"Understanding Subnetting to design addressing scheme"},{"content":"Introduction The Content-Security-Policy header adds the layer of security that facilitates web application administrators to control resources the user agent is allowed to load for a given page. This helps guard against cross-site scripting attacks (XSS), packet sniffing and data injection attacks.\nSyntax Content-Security-Policy: {policy-directive}; {policy-directive} Example Content-Security-Policy: default-src https: \u0026#39;unsafe-eval\u0026#39; \u0026#39;unsafe-inline\u0026#39;; object-src \u0026#39;none\u0026#39; Directive Terms Fetch directive Fetch directives control the locations from which certain resource types may be loaded. All fetch directives fall back to default-src.\nscript-src Directive specifies valid sources for JavaScript.\nNonce Usage for script elements Content Security Policy Level 2 Ideally, developers would avoid inline scripts entirely and whitelist scripts by URL. However, in some cases, removing inline scripts can be difficult or impossible For those cases, developers can whitelist scripts using a randomly generated nonce. Content-Security-Policy: default-src \u0026#39;self\u0026#39;; script-src \u0026#39;self\u0026#39; https://example.com \u0026#39;nonce-EDNnf03nceIOfn39fn3e9h3sdfa\u0026#39; \u0026lt;script nonce=\u0026#34;EDNnf03nceIOfn39fn3e9h3sdfa\u0026#34;\u0026gt; alert(\u0026#39;Still blocked because nonce is wrong.\u0026#39;); \u0026lt;/script\u0026gt; default-src Directive serves as a fallback for the other CSP fetch directives.\nstyle-src Directive specifies valid sources for stylesheets.\nframe-ancestors Directive specifies valid parents that may embed a page using \u0026lt;frame\u0026gt;, \u0026lt;iframe\u0026gt;, \u0026lt;object\u0026gt;, \u0026lt;embed\u0026gt;, or \u0026lt;applet\u0026gt;.\nobject-src Directive specifies valid sources for the \u0026lt;object\u0026gt;, \u0026lt;embed\u0026gt;, and \u0026lt;applet\u0026gt; elements.\nreport-to Directive instructs the user agent to store reporting endpoints for an origin.\nImplementing in AWS Amplify # customHTTP.yml customHeaders: - pattern: \u0026#39;**/*\u0026#39; headers: - key: \u0026#39;Strict-Transport-Security\u0026#39; value: \u0026#39;max-age=31536000; includeSubDomains\u0026#39; - key: \u0026#39;X-Frame-Options\u0026#39; value: \u0026#39;SAMEORIGIN\u0026#39; - key: \u0026#39;X-XSS-Protection\u0026#39; value: \u0026#39;1; mode=block\u0026#39; - key: \u0026#39;X-Content-Type-Options\u0026#39; value: \u0026#39;nosniff\u0026#39; - key: \u0026#39;Content-Security-Policy\u0026#39; value: \u0026#34;default-src \u0026#39;self\u0026#39;; style-src \u0026#39;self\u0026#39;; img-src \u0026#39;self\u0026#39; data:;\u0026#34; Troubleshooting Mitigating with CSP policy directive for corresponding errors:\nDirective 1: img-src \u0026lsquo;self\u0026rsquo; data: Refused to load the image \u0026#39;data:image/png;base64,somerandomnumber\u0026#39; because it violates the following Content Security Policy directive: \u0026#34;img-src \u0026#39;self\u0026#39; data\u0026#34; Directive 2: connect-src https://*.example1.com Refused to connect to \u0026#39;https://hello.example1.com/\u0026#39; because it violates the following Content Security Policy directive: \u0026#34;default-src \u0026#39;self\u0026#39;\u0026#34;. Note that \u0026#39;connect-src\u0026#39; was not explicitly set, so \u0026#39;default-src\u0026#39; is used as a fallback. Directive 3: connect-src wss://example.net Refused to connect to \u0026#39;wss://example.net\u0026#39; because it violates the following Content Security Policy directive. Directive 4: style-src-elem \u0026lsquo;self\u0026rsquo; \u0026lsquo;unsafe-inline\u0026rsquo; https://example.com Refused to load the stylesheet \u0026#39;https://example.com/custom.css\u0026#39; because it violates the following Content Security Policy directive: \u0026#34;style-src \u0026#39;self\u0026#39; \u0026#39;unsafe-inline\u0026#39;\u0026#34;. Note that \u0026#39;style-src-elem\u0026#39; was not explicitly set, so \u0026#39;style-src\u0026#39; is used as a fallback. \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34; href=\u0026#34;https://example.com/custom.css\u0026#34; /\u0026gt; Directive 5: font-src \u0026lsquo;self\u0026rsquo; data:; Refused specific: data:font/woff;base64,\u0026#34;someBase64encoded font\u0026#34;, Validation Tools CSP Validator (https://csp-evaluator.withgoogle.com/)\nCSP Scanner (https://csper.io/evaluator)\n","permalink":"http://localhost:1313/blogs/tech/http-security-with-csp-headers/","summary":"Key notes of CSP response headers and troubleshooting guide","title":"HTTP Security with Content-Security-Policy headers"},{"content":"Index Fragmentation Index Fragmentation percentage varies when the logical page orders don’t coordinate with the physical page order in the page allocation of an index. With the data modification in the table, information can be resized on the data page. Users can observe the disturbing page order with the massive delete operation on the table. Along with the update and delete operations, the data page won’t be a top-full or empty page. Therefore, non-utilized free space raises the order mismatch between logical page and physical page with increasing the fragmentation, and that can cause worst query performance and consumes more server resources as well. There can be a number of indexes created on a single table with the combination of various columns, and each index can have a different fragmentation percentage. Fragmentation Status using the inbuilt T-SQL statement SELECT S.name AS \u0026#39;Schema\u0026#39;, T.name AS \u0026#39;Table\u0026#39;, I.name AS \u0026#39;Index\u0026#39;, DDIPS.avg_fragmentation_in_percent, DDIPS.page_count FROM sys.dm_db_index_physical_stats(DB_ID(), NULL, NULL, NULL, NULL) AS DDIPS INNER JOIN sys.tables T ON T.object_id = DDIPS.object_id INNER JOIN sys.schemas S ON T.schema_id = S.schema_id INNER JOIN sys.indexes I ON I.object_id = DDIPS.object_id AND DDIPS.index_id = I.index_id WHERE DDIPS.database_id = DB_ID() AND I.name IS NOT NULL AND DDIPS.avg_fragmentation_in_percent \u0026gt; 0 ORDER BY DDIPS.avg_fragmentation_in_percent DESC Output Here, we can see the maximum fragmentation percentage is 94% in the local database right after loading dummy data. To reduce this fragmentation, either of these can be performed in the database:\nREBUILD INDEX The INDEX REBUILD always drops the index and reproduces it with new index pages which runs with the ALTER INDEX command. This activity can be run in parallel using an online option which does not affect the running requests and tasks of a similar table. In the case of REBUILD INDEX offline, then the object resource of the index won’t be accessible till the end of the REBUILD process completion.\n--Basic Rebuild Command ALTER INDEX Index_Name ON Table_Name REBUILD --REBUILD Index with ONLINE OPTION ALTER INDEX Index_Name ON Table_Name REBUILD WITH (ONLINE = ON) REORGANIZE INDEX The REORGANIZE INDEX command reorders the index page by expelling the free or unused space on the page. Ideally, index pages are reordered physically in the data file. REORGANIZE does not drop and create the index but simply restructures the information on the page. REORGANIZE does not have any offline choice, and REORGANIZE does not affect the statistics compared to the REBUILD option. REORGANIZE performs online always.\nALTER INDEX Index_Name ON Table_Name REORGANIZE SQL Server Index and Statistics Maintenance Link to Maintenance Solution: https://ola.hallengren.com/sql-server-index-and-statistics-maintenance.html\nIndexOptimize is the SQL Server Maintenance Solution’s stored procedure for rebuilding and reorganizing indexes and updating statistics. IndexOptimize is supported on SQL Server 2008, SQL Server 2008 R2, SQL Server 2012, SQL Server 2014, SQL Server 2016, SQL Server 2017, SQL Server 2019, Azure SQL Database, and Azure SQL Database Managed Instance.\nDownload MaintenanceSolution.sql. This script creates all the objects and jobs that you need. You can also download the objects as separate scripts. The SQL Server Maintenance Solution is available on GitHub.\n-- Rebuild or reorganize all indexes with fragmentation on database1 -- database2 and index more than 5% will be REORGANIZED and REBUILT -- whereas index more than 30% will only be REBUILT, update modified -- statistics and log the results to a table. EXECUTE dbo.IndexOptimize @Databases = \u0026#39;database1, database2\u0026#39;, @FragmentationLow = NULL, @FragmentationMedium = \u0026#39;INDEX_REORGANIZE,INDEX_REBUILD_ONLINE,INDEX_REBUILD_OFFLINE\u0026#39;, @FragmentationHigh = \u0026#39;INDEX_REBUILD_ONLINE,INDEX_REBUILD_OFFLINE\u0026#39;, @FragmentationLevel1 = 5, @FragmentationLevel2 = 30, @MinNumberOfPages = 1, @UpdateStatistics = \u0026#39;ALL\u0026#39;, @SortInTempdb = \u0026#39;Y\u0026#39;, @MaxDOP = 0, @LogToTable = \u0026#39;Y\u0026#39; Databases Select databases. The hyphen character (-) is used to exclude databases, and the percent character (%) is used for wildcard selection. All of these operations can be combined by using the comma (,).\nValue Description SYSTEM_DATABASES All system databases (master, msdb, and model) USER_DATABASES All user databases ALL_DATABASES All databases AVAILABILITY_GROUP_DATABASES All databases in availability groups USER_DATABASES, AVAILABILITY_GROUP_DATABASES All user databases that are not in availability groups Db1 The database Db1 Db1, Db2 The databases Db1 and Db2 USER_DATABASES, -Db1 All user databases, except Db1 %Db% All databases that have “Db” in the name %Db%, -Db1 All databases that have “Db” in the name, except Db1 ALL_DATABASES, -%Db% All databases that do not have “Db” in the name FragmentationLow / FragmentationMedium / FragmentationHigh Specify index maintenance operations to be performed on a low/medium/high-fragmented index. An online index rebuild or an index reorganization is not always possible. Because of this, you can specify multiple index-maintenance operations for each fragmentation group. These operations are prioritized from left to right: If the first operation is supported for the index, then that operation is used; if the first operation is not supported, then the second operation is used (if supported), and so on. If none of the specified operations are supported for an index, then that index is not maintained.\nValue Description INDEX_REBUILD_ONLINE Rebuild index online. INDEX_REBUILD_OFFLINE Rebuild index offline. INDEX_REORGANIZE Reorganize index. INDEX_REBUILD_ONLINE, INDEX_REBUILD_OFFLINE Rebuild index online. Rebuild index offline if online rebuilding is not supported on an index. INDEX_REBUILD_ONLINE, INDEX_REORGANIZE Rebuild index online. Reorganize index if online rebuilding is not supported on an index. INDEX_REORGANIZE, INDEX_REBUILD_ONLINE, INDEX_REBUILD_OFFLINE Reorganize index. Rebuild index online if reorganizing is not supported on an index. Rebuild index offline if reorganizing and online rebuilding are not supported on an index FragmentationLevel1 / FragmentationLevel2 Set the lower limit, as a percentage, for medium fragmentation. The default is 5 percent.\nSet the lower limit, as a percentage, for high fragmentation. The default is 30 percent.\nThis is based on Microsoft’s recommendation in Books Online. IndexOptimize checks avg_fragmentation_in_percent in sys.dm_db_index_physical_stats to determine the fragmentation.\nMinNumberOfPages / MaxNumberOfPages Set a size, in pages; indexes with fewer pages are skipped for index maintenance. The default is 1000 pages. This is based on Microsoft’s recommendation.\nSet a size, in pages; indexes with a greater number of pages are skipped for index maintenance. The default is no limitation.\nIndexOptimize checks page_count in sys.dm_db_index_physical_stats to determine the size of the index.\nUpdateStatistics Value Description ALL Update index and column statistics. INDEX Update index statistics. COLUMNS Update column statistics. NULL Do not perform statistics maintenance. This is the default. SortInTempdb Use tempdb for sort operations when rebuilding indexes. The SortInTempdb option in IndexOptimize uses the SORT_IN_TEMPDB option in the SQL Server ALTER INDEX command. Values: Y and N (default N ).\nMaxDOP Specify the number of CPUs to use when rebuilding indexes. If this number is not specified, the global maximum degree of parallelism is used.\nThe MaxDOP option in IndexOptimize uses the MAXDOP option in the SQL Server ALTER INDEX command.\nLogToTable Log commands to the table dbo.CommandLog . Values: Y and N (default N ).\nSELECT CONCAT(DatabaseName, \u0026#39;.\u0026#39;, SchemaName, \u0026#39;.\u0026#39;, ObjectName) AS DatabaseSchemaObjectName, CASE WHEN ObjectType = \u0026#39;U\u0026#39; THEN \u0026#39;USER_TABLE\u0026#39; WHEN ObjectType = \u0026#39;V\u0026#39; THEN \u0026#39;VIEW\u0026#39; END AS ObjectType, IndexName, CASE WHEN IndexType = 1 THEN \u0026#39;CLUSTERED\u0026#39; WHEN IndexType = 2 THEN \u0026#39;NONCLUSTERED\u0026#39; WHEN IndexType = 3 THEN \u0026#39;XML\u0026#39; WHEN IndexType = 4 THEN \u0026#39;SPATIAL\u0026#39; END AS IndexType, ExtendedInfo.value(\u0026#39;(ExtendedInfo/PageCount)[1]\u0026#39;, \u0026#39;int\u0026#39;) AS [PageCount], ExtendedInfo.value(\u0026#39;(ExtendedInfo/Fragmentation)[1]\u0026#39;, \u0026#39;float\u0026#39;) AS Fragmentation, PartitionNumber, CommandType, Command, StartTime, EndTime, IIF( DATEDIFF(SS, StartTime, EndTime) / (24 * 3600) \u0026gt; 0, CAST(DATEDIFF(SS, StartTime, EndTime) / (24 * 3600) AS NVARCHAR) + \u0026#39;.\u0026#39;, \u0026#39;\u0026#39; ) + LEFT(CAST(DATEADD(MS, DATEDIFF(MS, StartTime, EndTime), 0) AS TIME), 12) AS Duration, ErrorNumber, ErrorMessage FROM dbo.CommandLog WHERE CommandType = \u0026#39;ALTER_INDEX\u0026#39; ORDER BY DatabaseName; Reference https://www.sqlshack.com/how-to-identify-and-resolve-sql-server-index-fragmentation/ https://ola.hallengren.com/sql-server-index-and-statistics-maintenance.html ","permalink":"http://localhost:1313/blogs/tech/sql-index-monitoring-and-rebuilding/","summary":"Monitoring and Rebuilding Indexes in SQL","title":"SQL Index Monitoring and Rebuilding"},{"content":"Checking the performance of the query DECLARE @r INT; DECLARE @ts DATETIME; DECLARE @statement CURSOR; -- This is the type selected value of the query DECLARE @dummy AS INT; DECLARE @results TABLE (elapsed DECIMAL); SET @r = 0; WHILE @r \u0026lt; 5 BEGIN SET @statement = CURSOR FOR -- Replace this QUERY. SELECT id FROM employees; SET @r = @r + 1 SET @ts = current_timestamp; OPEN @statement; FETCH NEXT FROM @statement INTO @dummy; WHILE @@FETCH_STATUS = 0 BEGIN FETCH NEXT FROM @statement INTO @dummy; END; CLOSE @statement; DEALLOCATE @statement; INSERT INTO @results VALUES (DATEDIFF(MS, @ts, current_timestamp)); END; SELECT CAST(elapsed AS DECIMAL(10, 5)) AS elapsed FROM @results; Output elapsed (ms) 14773.00000 18177.00000 14020.00000 9120.00000 9450.00000 Benchmarking SQL Queries DECLARE @ts DATETIME; DECLARE @repeat INT = 100; DECLARE @r INT; DECLARE @i INT; DECLARE @dummy INT; DECLARE @statement1 CURSOR; DECLARE @statement2 CURSOR; DECLARE @results TABLE ( run INT, statement INT, elapsed DECIMAL ); SET @r = 0; WHILE @r \u0026lt; 5 BEGIN SET @r = @r + 1 SET @statement1 = CURSOR FOR -- Paste statement 1 here SELECT id FROM employee_comments; SET @statement2 = CURSOR FOR -- Paste statement 2 here SELECT id FROM employees; SET @ts = current_timestamp; SET @i = 0; WHILE @i \u0026lt; @repeat BEGIN SET @i = @i + 1 OPEN @statement1; FETCH NEXT FROM @statement1 INTO @dummy; WHILE @@FETCH_STATUS = 0 BEGIN FETCH NEXT FROM @statement1 INTO @dummy; END; CLOSE @statement1; END; DEALLOCATE @statement1; INSERT INTO @results VALUES (@r, 1, DATEDIFF(MS, @ts, current_timestamp)); SET @ts = current_timestamp; SET @i = 0; WHILE @i \u0026lt; @repeat BEGIN SET @i = @i + 1 OPEN @statement2; FETCH NEXT FROM @statement2 INTO @dummy; WHILE @@FETCH_STATUS = 0 BEGIN FETCH NEXT FROM @statement2 INTO @dummy; END; CLOSE @statement2; END; DEALLOCATE @statement2; INSERT INTO @results VALUES (@r, 2, DATEDIFF(MS, @ts, current_timestamp)); END; SELECT statement, run, CAST(CAST(elapsed / MIN(elapsed) OVER () AS DECIMAL(10, 5)) AS VARCHAR) AS elapsed_ratio FROM @results ORDER BY statement, run; Output statement run elapsed_ratio 1 1 100.46154 1 2 101.76923 1 3 91.53846 1 4 100.76923 1 5 92.07692 2 1 1.07692 2 2 1.30769 2 3 1.53846 2 4 1.00000 2 5 1.00000 ","permalink":"http://localhost:1313/blogs/tech/sql-benchmarking/","summary":"Script that benchmark the given queries.","title":"SQL performance benchmarking queries"},{"content":"Leaptalk Resources https://github.com/cham11ng/decorators-and-mixin-classes https://github.com/RobusGauli/jsonvalidate ","permalink":"http://localhost:1313/slides/decorators-and-mixin-classes-in-python/","summary":"Leaptalk on understanding decorators, mixin classes and real-world implementation","title":"Decorators and Mixin Classes in Python"},{"content":" Credit: Anup Dhakal (https://anupdhakal.com)\nIntroduction In this guide, LEPP stands for Linux, Nginx (pronounced as Engine-X) , Postgres and PHP (PHP Hypertext Preprocessor).\nSpecifications/Target Ubuntu v16.04 Nginx v1.10 PHP v7.1 Postgres v9.6.3 phpPgAdmin v5.2 But before we start, let\u0026rsquo;s quickly make sure that we have some basic tools ready. Run the following commands in the terminal.\nsudo apt update sudo apt install wget sudo apt install software-properties-common Now let\u0026rsquo;s start!\nCustom Repositories Before beginning the installation, we want to add some repositories which will give us the latest corresponding packages for our server stack.\nNginx Repository For Ubuntu, in order to authenticate the Nginx repository signature and to eliminate warnings about missing PGP key during installation of the Nginx package, it is necessary to add the key used to sign the Nginx packages and repository to the apt program keyring. Only after then we will dare to add the repositories.\nThe above fact is true every time we add a custom repository. If we don\u0026rsquo;t want to add any custom repository and use the ones provided to us by \u0026ldquo;vanilla\u0026rdquo; Ubuntu itself, we can just ignore this section of the guide, entirely. We have to keep in mind that the version numbers used in this guide are according to the default, most recent versions of the respective packages at the time of creation of this guide. So, we might have to be careful in coming sections of the guide where we start installation and setup of the stack. We would want to make sure then, that the version number would match to what we have installed, not what is shown in this guide.\nRun the following code one by one in the terminal to add the Nginx stable repository. We need to accept any prompts, if asked.\nwget http://nginx.org/keys/nginx_signing.key sudo apt-key add nginx_signing.key sudo sed -i \u0026#39;$a deb http://nginx.org/packages/ubuntu/ xenial nginx\u0026#39; /etc/apt/sources.list sudo sed -i \u0026#39;$a # deb-src http://nginx.org/packages/ubuntu/ xenial nginx\u0026#39; /etc/apt/sources.list Postgres Repository The PostgreSQL apt repository supports LTS versions of Ubuntu 16.04 on amd64 and i386 architectures. This repository will integrate with your normal systems and patch management, and provide automatic updates for all supported versions of PostgreSQL throughout the support lifetime of PostgreSQL.\nCreate the file /etc/apt/sources.list.d/pgdg.list, and add a line for the repository deb http://apt.postgresql.org/pub/repos/apt/ xenial-pgdg main Import the repository signing key, and update the package lists wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | \\ sudo apt-key add - PHP Repository The repository here we are going to install is the PHP PPA by Ondrej. We can update our system with unsupported packages from this untrusted PPA by adding ppa:ondrej/php to our system\u0026rsquo;s Software Sources. The following commands will help us to do so. We need to accept any prompts, if asked.\nsudo add-apt-repository ppa:ondrej/php We might feel a little bit confused by the terms \u0026ldquo;untrusted PPA\u0026rdquo;, but there is actually no reason to worry. We just need to remember that, unlike Nginx\u0026rsquo;s or the MariaDB\u0026rsquo;s repositories, this is not an official upgrade path. But the PPA is well known, and is relatively safe to use.\nUpdate APT Let\u0026rsquo;s finally begin the actual installations processes.\nFirst, we want to make sure we have the latest records in our local packages registry. Let\u0026rsquo;s run the following command in the terminal like so.\nsudo apt update Installing Nginx First thing we’re going to install is the server called Nginx.\nsudo apt install nginx We can check if Nginx is installed by typing nginx -v in the terminal.\nInstalling Postgres sudo apt-get install postgresql postgresql-contrib We can check if Postgres is installed by executing psql --version in the terminal.\nPHP makes use of the package php-pgsql as the database driver for Postgres.\nInstalling PHP Next thing we want to install is PHP. We need to install PHP with a few extensions that are mandatory for modern web applications.\nsudo apt install php-cli php-fpm php-zip php-xml php-mbstring php-mcrypt php-curl php-gd php-pgsql php-bz2 php-gettext php-pear php-phpseclib php-tcpdf We may run php -v in the terminal to check the version of PHP installed.\nConfiguring Nginx We don\u0026rsquo;t need to change anything right now. But if we want to, we can change the main configuration of Nginx as follows:\nsudo gedit /etc/nginx/nginx.conf In the config file, notice the line with user nginx. This means Nginx will run as the user nginx. We should not forget to run sudo systemctl restart nginx.service if we make changes to any configuration file of Nginx.\nConfiguring PHP to work with Nginx In order for PHP and Nginx to work together, we need to configure both of them. We need to make sure that PHP-FPM (FastCGI Process Manager) runs as the same user as Nginx. And for that we need to run sudo gedit /etc/php/7.1/fpm/pool.d/www.conf in the terminal and change the relevant lines as:\n... user = nginx group = nginx ... listen.owner = nginx listen.group = nginx ... Note the command where we used \u0026hellip; php/7.1/fpm \u0026hellip; . We want to make sure that 7.1 is the version that we actually have installed in our system. Refer this section to go back and see how we installed PHP and PHP-FPM, and how to see the version of PHP installed.\nWe need to run sudo systemctl restart php7.1-fpm.service in the terminal to load the new configuration.\nDefault Site Now comes the fun part where we create a default site that supports PHP. In our case, we want ~/www as our directory of all websites. Normally, /var/www is used as the default one. Here we want to change it to a custom directory inside our home directory, as mentioned above.\nFirst, we want to make sure the directory exists. Let\u0026rsquo;s create a default site directory _default_ with command mkdir -p ~/www/_default_/public.\nNote that to add other sites, we can follow a similar pattern. We may create a new folder for each site, which has a public folder in it as the public entry point of the site.\nNow to edit the default site configuration, let\u0026rsquo;s run the following command in the terminal\nsudo gedit /etc/nginx/conf.d/default.conf and replace its content with this:\nserver { listen 80 default_server; listen [::]:80 default_server; root /home/[OUR_USERNAME]/www/_default_/public; index index.html index.htm index.php; server_name _; location / { try_files $uri $uri/ =404; autoindex on; } location ~ \\.php$ { try_files $uri =404; fastcgi_split_path_info ^(.+\\.php)(/.+)$; fastcgi_pass unix:/run/php/php7.1-fpm.sock; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; } } Please note that we need to change [OUR_USERNAME] from above config file to our actual username.\nNow let\u0026rsquo;s run sudo systemctl restart nginx.service to reload our new configuration.\nNow we should be able to see on browser that http://localhost actually works, and most probably shows an empty index.\n**Ba Dum Tis**\nWe can put any file in the ~/www/_default_/public directory and it should be showing in the browser after a refresh.\nConfiguring Postgres Switch over to the postgres account on your server by typing:\nuser@aspire:~$ sudo -i -u postgres Let\u0026rsquo;s Create a New Role\npostgres@aspire:~$ createuser --interactive -P Enter name of role to add: {role name} Enter password for new role: Enter it again: Shall the new role be a superuser? (y/n) Installing phpPgAdmin Setting Up Host We want to be able to lunch phpPgAdmin by going to http://phppgadmin.app in the address bar of the browser.\nSo, first of all, run sudo gedit /etc/hosts and add an entry as follows:\n127.1.1.1 phppgadmin.app phpPgAdmin Run the following commands to install phpmyadmin:\ncd /usr/share sudo git clone git@github.com:phppgadmin/phppgadmin.git cd phppgadmin/config sudo cp config.inc.php-dist config.inc.php If extra login security is true, then logins via phpPgAdmin with no password or certain usernames (pgsql, postgres, root, administrator) will be denied. Only set this false once you have read the FAQ and understand how to change PostgreSQL\u0026rsquo;s pg_hba.conf to enable passworded local connections.\n$conf[\u0026#39;servers\u0026#39;][0][\u0026#39;host\u0026#39;] = \u0026#39;localhost\u0026#39;; $conf[\u0026#39;extra_login_security\u0026#39;] = false; Setting Up Server Block Now, we will setup an Nginx server block (a.k.a. virtual host in Apache httpd).\nLet\u0026rsquo;s run the command sudo touch /etc/nginx/conf.d/phppgadmin.conf to create a site (configuration file).\nLet\u0026rsquo;s open it with sudo gedit /etc/nginx/conf.d/phppgadmin.conf and add the contents as follows.\nserver { listen 80; root /usr/share/phppgadmin; # make sure to enter the correct location of phppgadmin here index index.php index.html index.htm; server_name phppgadmin.app; location / { try_files $uri $uri/ /index.php?$query_string; } location ~ \\.php$ { try_files $uri /index.php =404; fastcgi_split_path_info ^(.+\\.php)(/.+)$; fastcgi_pass unix:/run/php/php7.1-fpm.sock; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; } } Activating the server block Now we have to restart Nginx to activate our new site (config file) by running:\nsudo systemctl restart nginx.service Try hitting http://phppgadmin.app and it should work!\nConclusion There we have it! We should by now have a working and relatively secure LEPP server stack with Nginx running at http://localhost, as well as our phpPgAdmin app running at http://phppgadmin.app\nIn this guide we didn\u0026rsquo;t talk anything about firewall. This is because a fresh install of \u0026ldquo;vanilla\u0026rdquo; Ubuntu 16.04 should not have one running it automatically. We may research about it later if we wish to. Right now, that would be beyond the scope of this guide.\nSo, did you find this guide helpful? Feedbacks are precious. Suggestions are highly appreciated.\n","permalink":"http://localhost:1313/blogs/tech/linux-server-setup/","summary":"Learn to setup LEPP manually in Ubuntu Server.","title":"Linux Server Stack Setup in Ubuntu 16.04 LTS"},{"content":"Final Presentation Project Demo GitHub Link https://github.com/cham11ng/face-recognition ","permalink":"http://localhost:1313/slides/face-recognition-with-local-binary-patterns/","summary":"\u003ch2 id=\"final-presentation\"\u003eFinal Presentation\u003c/h2\u003e\n\u003cdiv\n  style=\"padding-bottom:56.25%; position:relative; display:block; width: 100%\"\u003e\n  \u003ciframe\n    width=\"100%\"\n    height=\"100%\"\n    src=\"https://docs.google.com/presentation/d/e/2PACX-1vRZtiEc3rShTvN7Giuz_dl4jyCGa6VdcK0wptwp6K3f_pNUqtASJs8Bd-FTOxgf8qAi3-LK2cLRndwV/embed?start=false\"\n    frameborder=\"0\"\n    scrolling=\"no\"\n    allowfullscreen=\"true\"\n    mozallowfullscreen=\"true\"\n    webkitallowfullscreen=\"true\"\n    style=\"position:absolute; top:0; left: 0\"\u003e\u003c/iframe\u003e\n\u003c/div\u003e\n\n\u003ch2 id=\"project-demo\"\u003eProject Demo\u003c/h2\u003e\n\u003cdiv style=\"position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;\"\u003e\n      \u003ciframe allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen=\"allowfullscreen\" loading=\"eager\" referrerpolicy=\"strict-origin-when-cross-origin\" src=\"https://www.youtube-nocookie.com/embed/x_G2NKEcz2E?autoplay=0\u0026amp;controls=1\u0026amp;end=0\u0026amp;loop=0\u0026amp;mute=0\u0026amp;start=0\" style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;\" title=\"YouTube video\"\u003e\u003c/iframe\u003e\n    \u003c/div\u003e\n\n\u003ch2 id=\"github-link\"\u003eGitHub Link\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/cham11ng/face-recognition\"\u003ehttps://github.com/cham11ng/face-recognition\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","title":"Face Recognition with Local Binary Patterns"},{"content":"Basic Design is solution to problem. 1997-2004 called web design. 2005-later called UI/UX. UI/UX is science. UX ? Why not UE? UX is responsible for secure feeling. Identify KiKi and BoBo. UI is looks and UX is usability. Design product such that it can be used without user manual. Good vs Bad Design Human Computer Interaction (HCI). Selection of chair. Revolving chair is not placed in meeting room. Change shape in case of different products. (e.g. Shampoo/Conditioner) Don\u0026rsquo;t change conceptual model. Affordance Get real time user\u0026rsquo;s feedback. Aesthetics of a site. Less is more Elements of Design Line link/highlights -\u0026gt; a header -\u0026gt; h1 list/menu/table -\u0026gt; ul/li/table Separator (different topic) -\u0026gt; hr panel Shape menu, radio, checkbox, tabs, textarea, textbox, dropdown, layout Direction Size Texture Color Principle of Design Repetition Alignment pixel perfect Proximity visual connection Space let the content breath Balance Contrast juxtaposition of opposing element Common Fate Synesthesia Exercise What did you see? How it tastes like? What it smells like? ","permalink":"http://localhost:1313/blogs/tech/dive-into-design-basics/","summary":"Learn few things about design basics.","title":"Dive into Design Basics"},{"content":"Installation For Linux (Debian/Ubuntu) sudo apt install git We can check if Git is installed by typing git --version in your Terminal or Git Bash (Windows) respectively.\nFor Windows Download Git for Windows from this link https://git-scm.com/downloads\nConfiguration Set up SSH for Git Open your Terminal or Git Bash(Windows).\nEnsure we have SSH client installed: $ ssh -v usage: ssh [-1246AaCfGgKkMNnqsTtVvXxYy] [-b bind_address] [-c cipher_spec] [-D [bind_address:]port] [-E log_file] [-e escape_char] [-F configfile] [-I pkcs11] [-i identity_file] [-L address] [-l login_name] [-m mac_spec] [-O ctl_cmd] [-o option] [-p port] [-Q query_option] [-R address] [-S ctl_path] [-W host:port] [-w local_tun[:remote_tun]] [user@]hostname [command] List the contents of your ~/.ssh directory.\n$ ls -a ~/.ssh known_hosts If we have already setup default identity, we can see two id_ files\n$ ls -a ~/.ssh . .. id_rsa id_rsa.pub known_hosts Set up our default identity: The system adds keys for all identities to the /home/\u0026lt;yourname\u0026gt;/.ssh (Linux) or /c/Users/\u0026lt;yourname\u0026gt;/.ssh (Windows) directory. The following procedure creates a default identity.\nHit ssh-keygen command on Terminal or Git Bash.\n$ ssh-keygen Generating public/private rsa key pair. Enter file in which to save the key (/c/Users/\u0026lt;yourname\u0026gt;/.ssh/id_rsa): Press enter to accept the default key and location or we can set somewhere else.\nWe enter and re-enter a passphrase when prompted. Unless you need a key for a process such as script, you should always provide a passphrase. The command creates your default identity with its public and private keys\n$ ssh-keygen Generating public/private rsa key pair. Enter file in which to save the key (/c/Users/\u0026lt;yourname\u0026gt;/.ssh/id_rsa): Created directory \u0026#39;/c/\u0026lt;yourname\u0026gt;/\u0026lt;yourname\u0026gt;/.ssh\u0026#39;. Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /c/Users/\u0026lt;yourname\u0026gt;/.ssh/id_rsa. Your public key has been saved in /c/Users/\u0026lt;yourname\u0026gt;/.ssh/id_rsa.pub. The key fingerprint is: SHA256:+V6cLhFWngIFC+TdHwQHxJP39lsZUWnP6TDj1/7hBVA \u0026lt;yourname\u0026gt;@\u0026lt;yourname\u0026gt; The key\u0026#39;s randomart image is: +---[RSA 2048]----+ | .o .==+o E +| | . o.o+oo. + | | . o..=oo..+| | .+.o*ooo| | S. oo.*.+| | ... o =+| | ..+ .o+| | ..o ..+| | ... .o| +----[SHA256]-----+ List the contents of your ~/.ssh directory.\n$ ls ~/.ssh . .. id_rsa id_rsa.pub known_hosts Adding your SSH key to the ssh-agent: Ensure ssh-agent is enabled:\n$ eval \u0026#34;$(ssh-agent -s)\u0026#34; # Agent pid 59566 Add your SSH key to the ssh-agent\nssh-add ~/.ssh/id_rsa Find and take a note of your public key fingerprint. If you\u0026rsquo;re using OpenSSH 6.7 or older: $ ssh-add -l # 2048 a0:dd:42:3c:5a:9d:e4:2a:21:52:4e:78:07:6e:c8:4d /Users/\u0026lt;yourname\u0026gt;/.ssh/id_rsa (RSA) If you are using OpenSSH 6.8 or newer: $ ssh-add -l -E md5 # 2048 MD5:a0:dd:42:3c:5a:9d:e4:2a:21:52:4e:78:07:6e:c8:4d /Users/\u0026lt;yourname\u0026gt;/.ssh/id_rsa (RSA) Install the public key on your Remote GitHub/Bitbucket account: $ cat ~/.ssh/id_rsa.pub ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC2zeZVIph1tP0UZJ007AC1OWqThpYjDlao1PlQnZbrSMeS8LXkU/nMxuZdAv+2JeqhezOtb6/e8e50NOTWB9Z2O8thCMwc29cp6C+vHL2oWQYMcCOuT34/R2yDEOMQ5nkIZ1fVFJNCTIZUaKjyaHX89w0v2p9cMsZ1q36w9lEdKXs8N5fuN/6rAy3JQgMcbD+dDd0cWpP8CLiUyNCq32xwqhX+nS1P43AgOQdLpX74uljwr7rE2CmrJQkvh/m+h68tv8+mLMGJtg5cJ+doZ+9r9yPhKJYGEsW4bL+8sSRQn3gJWUib8xhOgaWrMfXj+94o1KbcI12lK772GNyP74rX \u0026lt;yourname\u0026gt;@\u0026lt;yourname\u0026gt; Copy this output to respective SSH keys setting.\nGitHub Setting \u0026raquo; SSH and GPG keys \u0026raquo; New SSH key Bitbucket Setting \u0026raquo; SSH keys \u0026raquo; Add Key You are Done You have now successfully configured SSH for Git in Windows/Linux/Mac OS.\nFirst Time Git Setup This is my configuration.\n$ git config --global user.name \u0026#34;Sagar Chamling\u0026#34; $ git config --global user.email sgr.raee@gmail.com $ git config --global core.editor vim $ git config --global commit.gpgsign true $ git config --global pull.rebose false gpg --list-secret-keys --keyid-format LONG /Users/hubot/.gnupg/secring.gpg ------------------------------------ sec 4096R/3AA5C34371567BD2 2016-03-10 [expires: 2017-03-10] uid Hubot ssb 4096R/42B317FD4BA89E7A 2016-03-10 $ git config --global user.signingkey 3AA5C34371567BD2 # Remember GPG password when signing git commits $ vim ~/.gnupg/gpg-agent.conf $ default-cache-ttl 3600 References https://help.github.com/articles/connecting-to-github-with-ssh https://confluence.atlassian.com/bitbucket/set-up-ssh-for-git-728138079.html https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/telling-git-about-your-signing-key https://stackoverflow.com/questions/36847431/remember-gpg-password-when-signing-git-commits ","permalink":"http://localhost:1313/blogs/tech/git-installation-and-configure-ssh/","summary":"Guide to install and configure SSH in Windows and Linux.","title":"Git Installation and Configure SSH"},{"content":"","permalink":"http://localhost:1313/projects/","summary":"","title":"Projects"}]